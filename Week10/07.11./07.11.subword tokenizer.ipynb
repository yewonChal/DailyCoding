{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6164ec-f07a-4473-8a9a-3281c4b6f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword Tokenizer을 활용하여 간편하게 분리 진행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce87c37c-e8b6-4c4e-9d55-4eaade52748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('IMDb_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292b2393-64ae-4722-a4d3-575f809de199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3954377f-b7f2-418d-9fe5-da8aa4003226",
   "metadata": {},
   "source": [
    "! pip3 install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7e9e3dc-a13d-420e-ad8d-a92d268dd981",
   "metadata": {},
   "source": [
    "conda install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8efa105-db2e-4852-8ab3-e062810ef0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5303d5-d6d6-4d7e-8d45-4f53a51504c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54bac1ea-24fa-4d88-a5dd-6491ec6dfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52702e59-28ca-42eb-9367-ffc3f58c1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58998e50-8b9f-454a-a1ad-b4fda4fe5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84cb952d-97e6-438a-bb53-dceda2f97b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(train_df['review'][20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "965ad9a4-2399-48a2-b0f3-203e23901621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(train_df['review'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "066f094b-1987-4c35-89b3-f8560b12a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후 :  [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
      "원본 디코딩 후 :  It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후 : ', tokenized_string)\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('원본 디코딩 후 : ', original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc3067ba-ae82-4451-a732-3f8d2f3ad3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 :  8268\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기 : ', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76cd4884-0961-477a-9294-46e895f96542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ------> It\n",
      "8051 ------> '\n",
      "8 ------> s \n",
      "910 ------> mind\n",
      "8057 ------> -\n",
      "2169 ------> blow\n",
      "36 ------> ing \n",
      "7 ------> to \n",
      "103 ------> me \n",
      "13 ------> that \n",
      "14 ------> this \n",
      "32 ------> film \n",
      "18 ------> was \n",
      "79 ------> even \n",
      "681 ------> made\n",
      "8058 ------> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print('{} ------> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0674fbcb-2883-4e1b-a624-0ef722cfceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후 :  [137, 8051, 8, 910, 8057, 2169, 100, 8132, 8133, 997, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
      "원본 디코딩 후 :  It's mind-blowingxyz to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"It's mind-blowingxyz to me that this film was even made.\"\n",
    "# 의미없는 알파벳을 이어 붙였을 때\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후 : ', tokenized_string)\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('원본 디코딩 후 : ', original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e98fcb0-8950-4bc8-8a43-ea579ca7f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ------> It\n",
      "8051 ------> '\n",
      "8 ------> s \n",
      "910 ------> mind\n",
      "8057 ------> -\n",
      "2169 ------> blow\n",
      "100 ------> ing\n",
      "8132 ------> x\n",
      "8133 ------> y\n",
      "997 ------> z \n",
      "7 ------> to \n",
      "103 ------> me \n",
      "13 ------> that \n",
      "14 ------> this \n",
      "32 ------> film \n",
      "18 ------> was \n",
      "79 ------> even \n",
      "681 ------> made\n",
      "8058 ------> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print('{} ------> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "952350b7-c9ef-48a7-a416-8d2ad7ff4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b0933e0-17a8-4cd9-ba75-864743eb3330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d035eab-a333-4419-9c0c-d2f4a659609a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16e78cb8-59d7-430c-aca9-cea2d8211a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().values.any()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ed4efb8-52c5-438f-afe1-1e231e00d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True : 빈값(결측값) 존재)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ef19930-bcde-44c7-ba49-b7a3345c5c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna()\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfd9165c-a3ea-438a-a2c6-2ca70afbdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4d487c9-fe8f-469f-b271-c97ffaa9549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "152ac7d2-4174-4374-81e8-9f17d081ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절대 평범한 영화가 아닌 수작이라는걸 말씀드립니다.\n"
     ]
    }
   ],
   "source": [
    "print(train_data['document'][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "926d9cc8-7f03-46ac-9ee0-b9e6e631e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[749, 3004, 84, 633, 4801, 1018, 329, 194, 2432, 2042, 8044]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(train_data['document'][23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "191775f3-c062-4a47-ac59-92334132ff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후 :  [3, 549, 18, 308, 956, 678, 3106, 9, 8044]\n",
      "원본 디코딩 후 :  영화시간이 너무 길어서 보기 힘들었다.\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"영화시간이 너무 길어서 보기 힘들었다.\"\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후 : ', tokenized_string)\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('원본 디코딩 후 : ', original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5808f13c-9720-45b8-9ca2-460fe964daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후 :  [1839, 1567, 35, 164, 847, 782, 2158, 156, 2799, 6611, 9, 8044]\n",
      "원본 디코딩 후 :  성인돼서 다시 보니 조금 유치한 것 같아서 졸았다.\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"성인돼서 다시 보니 조금 유치한 것 같아서 졸았다.\"\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('정수 인코딩 후 : ', tokenized_string)\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print('원본 디코딩 후 : ', original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b81f878-2810-44b3-9de2-f5728199d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839 ------> 성인\n",
      "1567 ------> 돼\n",
      "35 ------> 서 \n",
      "164 ------> 다시 \n",
      "847 ------> 보니 \n",
      "782 ------> 조금 \n",
      "2158 ------> 유치한 \n",
      "156 ------> 것 \n",
      "2799 ------> 같아서 \n",
      "6611 ------> 졸았\n",
      "9 ------> 다\n",
      "8044 ------> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print('{} ------> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa677c9c-b74b-4343-b442-721068c7858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyyaml, fsspec, huggingface-hub, tokenizers\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 pyyaml-6.0.1 tokenizers-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3847a787-50c9-4fe0-9813-18981076764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c50a9108-de33-4f98-b7e1-3602c48f3cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_df = pd.read_table('ratings.txt')\n",
    "naver_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eafda37c-3b32-4d62-bdfc-4d1f7adacd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "naver_df = naver_df.dropna()\n",
    "print(naver_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed53d09e-fad1-4463-a5dc-6ca53cb460a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naver_review.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31d3fd3e-c8e0-4285-9c20-8e4463deccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eada7a6b-a6a0-42a2-b7c5-6220276b7c47",
   "metadata": {},
   "source": [
    "-> lowercase : True인 경우 tokenizer는 영어의 대문자와 소문자를 동일한 문자 취급\n",
    "-> strip_accents : True인 경우 악센트 제거 (é → e, ô → o)\n",
    "\n",
    "-> wordpieces_prefix : 서브워드의 경우 분리되었을 때 뒤에 나오는 서브워드 앞에 ##을 붙여서 분리된 단어임을 표시한다.\n",
    "ex) 외국디자이너 -> 외국, ##디자이너"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f899e30-daf7-4730-851d-ab284ca8e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'naver_review.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "# limit_alphabet : merge가 되지 않은 초기 토큰(문자 단위)의 허용 제한 개수\n",
    "# min_frequency : merge을 위한 pair의 최소 등장 횟수\n",
    "tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "340232fb-41f3-4e81-bbbb-11552868ff43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3535d778-1a74-48d9-a144-b8d232499cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>맘을</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>맛도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>망하지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>망한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>망가지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "29995   맘을\n",
       "29996   맛도\n",
       "29997  망하지\n",
       "29998  망한다\n",
       "29999  망가지"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf('vocab.txt', header=None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b89bb81e-b68d-497e-9423-aadffa500239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 :  ['성인', '##돼서', '다시', '보니', '조금', '유치한', '것', '같아서', '졸았다', '.']\n",
      "정수 인코딩 :  [8183, 20803, 5895, 6758, 6190, 7258, 612, 8572, 15936, 18]\n",
      "디코딩 :  성인돼서 다시 보니 조금 유치한 것 같아서 졸았다.\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('성인돼서 다시 보니 조금 유치한 것 같아서 졸았다.')\n",
    "print('토큰화 결과 : ', encoded.tokens)\n",
    "print('정수 인코딩 : ', encoded.ids)\n",
    "print('디코딩 : ', tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46e04fa9-9990-4c74-87e8-d12ab1766fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2631b520-2f3a-47db-bdc0-26fca344fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['▁성인', '돼', '서', '▁다시', '▁보니', '▁조금', '▁유치한', '▁것', '▁같아서', '▁졸', '았다.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode('성인돼서 다시 보니 조금 유치한 것 같아서 졸았다.')\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56980e2-d775-4c13-820e-9068fff16db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
