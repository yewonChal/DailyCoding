{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e69a78-c5ec-4a45-8751-78cdfaceb09a",
   "metadata": {},
   "source": [
    "# 시퀸스-투-시퀸스(Sequence-to-Sequence, Seq2Seq)\n",
    ": 하나의 시퀸스를 입력받아 또 다른 시퀸스를 출력하는 모델\n",
    "\n",
    "1. 인코더(Encoder) : 입력 시퀸스를 고정된 길이의 컨텍스트 벡터(Context Vector)로 변환한다.\n",
    "- RNN, LSTM, GRU와 같은 순환 신경망이 사용된다.\n",
    "- 인코더는 입력 시퀀스를 시간 단계별로 처리하여 각 단계의 은닉 상태를 생성한다.\n",
    "\n",
    "2. 디코더(Decoder) : 인코더에서 생성된 컨텍스트 벡터를 사용하여 시퀀스를 생성한다.\n",
    "- RNN, LSTM, GRU와 같은 순환 신경망이 사용된다.\n",
    "- 인코더의 마지막 은닉 상태를 초기 상태로 사용하고, 이전 단계의 출력을 다음 시간 단계의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e234d02c-e4bf-4f8e-8639-1fc3b9301b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "url = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "filename = 'spa-eng.zip'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2315ea2-e54f-4e22-bd11-9bac2b5a8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'spa-eng/spa.txt'\n",
    "\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split('\\t')\n",
    "    spanish = '[start] ' + spanish + ' [end]'\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31923f69-19a5-432b-90b5-bcb22dfe465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Tom's new girlfriend is quite attractive.\", '[start] La nueva novia de Tom es bastante atractiva. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295c50c4-bc9a-4c71-afaa-97b8241421c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607dcf01-0965-479c-8100-ccffd0f6ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83276 17844 17844\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pairs), len(val_pairs), len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e0091e2-ec84-4f73-b071-adfbd1938aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어와 스페인어 벡터화 처리\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "import re\n",
    "\n",
    "# 특수문자 제거(제거할 특수문자 지정)\n",
    "strip_chars = string.punctuation + '¿'\n",
    "strip_chars = strip_chars.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')\n",
    "\n",
    "# 사용자 정의 전처리 함수\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string) # 문자열을 소문자로 변환\n",
    "    return tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '') # 특수문자 제거\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length + 1, # Q. 스페인어에만 +1이 들어가 있는 이유\n",
    "    # A. 시작 토큰 추가, 종료 토큰 예측\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b0867de-42d0-40ca-bc6c-f2ce7e073195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Tom didn't feel well, but he went to work anyway.\",\n",
       " '[start] Tom no se sentía bien, pero de todos modos fue a trabajar. [end]')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_english_texts[0], train_spanish_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cf9cd49-eacc-49e6-be46-e892d06aa8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        'english':eng,\n",
    "        'spanish':spa[:, :-1], # 종료 토큰 제외 (Seq2Seq 모델의 입력)\n",
    "    }, spa[:, 1:]) # 시작 토큰 제외 (Seq2Seq 모델의 출력 타겟)\n",
    "\n",
    "def make_dataset(paris):\n",
    "    eng_texts, spa_texts = zip(*paris)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts)) # 각각 텐서 플로우 데이터 셋으로 변환\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
    "    return dataset.shuffle(42).prefetch(16).cache() # 데이터 셋 셔플, prefetch(16) : 16개의 배치를 미리 가져오고 캐싱을 수행한다.\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac33ece9-feb0-473f-ae11-253887eee37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape : (64, 20)\n",
      "inputs['spanish'].shape : (64, 20)\n",
      "targets.shape : (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 14:21:30.938832: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape : {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape : {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "859fdacd-6674-4a2f-a475-809d50b00485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None, ), dtype='int64', name='english')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode='sum')(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18817764-bd61-4a35-ac86-fcaf018dfc95",
   "metadata": {},
   "source": [
    "past_target = keras.Input(shape=(None, ), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "x, _ = decoder_gru(x, initial_state=state_h)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "target_next_step = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298ca62b-8b31-4fa4-bbe0-e52998e370ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " english (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " spanish (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)     (None, None, 256)            3840000   ['english[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)     (None, None, 256)            3840000   ['spanish[0][0]']             \n",
      "                                                                                                  \n",
      " gru_6 (GRU)                 [(None, 512),                1182720   ['embedding_6[0][0]']         \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " gru_7 (GRU)                 [(None, None, 512),          1182720   ['embedding_7[0][0]',         \n",
      "                              (None, 512)]                           'gru_6[0][1]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, None, 512)            0         ['gru_7[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 15000)          7695000   ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17740440 (67.67 MB)\n",
      "Trainable params: 17740440 (67.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define constants (example values, you should set these appropriately)\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "embed_dim = 256\n",
    "latent_dim = 512\n",
    "\n",
    "# Define the source input\n",
    "source = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "\n",
    "# Define the encoder (this should be previously defined)\n",
    "# Assuming encoded_source is obtained from an encoder model\n",
    "# Example encoder definition (should be replaced with your actual encoder model)\n",
    "encoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(source)\n",
    "encoder_gru = layers.GRU(latent_dim, return_state=True)\n",
    "encoded_source, state_h = encoder_gru(encoder_embedding)\n",
    "\n",
    "# Define the past target input\n",
    "past_target = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "\n",
    "# Define the decoder GRU\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "x, _ = decoder_gru(x, initial_state=state_h)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Define the output layer\n",
    "target_next_step = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "# Define the seq2seq model\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
    "\n",
    "# Print the model summary\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bfdd4-ed2b-4e64-b7bb-cba17e9e7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'sparse_catehorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "seq2seq_rnn.fit(train_df, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "207e4005-ab92-48a1-bb95-fda9209f4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Input sentence: Stop moving!\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah ríes extremo pagando traigas naciste científico reconocería vampiros trago rinda sentaron dándose estupendo jugó abordaron terminarme tiritando usemos\n",
      "---------------------------------------------\n",
      "Input sentence: This CD belongs to my son.\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "Decoded sentence: [start] ¡aléjate prisión revisas almorzando álbum abandonada respondé ipod presidenta benjamín echaras ¡apúrate incertidumbre antoja pesadillas voltios contenía inapropiado podido pegué\n",
      "---------------------------------------------\n",
      "Input sentence: Tom is generous and kind.\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "Decoded sentence: [start] jugaban empeorar atrapar hambre hipoteca aspecto ausente flotar trofeo vístete conocido evitarse tráete nombró importar bar tócalo guárdame enseño abrace\n",
      "---------------------------------------------\n",
      "Input sentence: I didn't know Tom couldn't speak French.\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Decoded sentence: [start] terminamos vuélvelo drásticamente subió ¡peligro ¡mostrame abogada permanezca seleccionados ingresó llevo ¡peligro circunstancias rondé suelen adoran retrospectiva dividida guerrero llegué\n",
      "---------------------------------------------\n",
      "Input sentence: I can resist everything but temptation.\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "Decoded sentence: [start] terminamos separado 160 déjale sahara mamíferos apagué recuerdo comerciales sede expectativas quiero huevo americanos descubrieron intereses vuelva prescindir silbido diciembre\n",
      "---------------------------------------------\n",
      "Input sentence: Tom speaks French as well as I do.\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah talaron digas digas perfil piel salen neumáticos cogerá casi seguridad tokyo riesgo atrasado construyendo carretero hacía lamentará viaja\n",
      "---------------------------------------------\n",
      "Input sentence: He was cured of his bad habits.\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah ríes extremo parezca romance romance viajar adoptado acto ícono impresora empiezo sé terrible enamorados reinó esperé aparentemente aprendería\n",
      "---------------------------------------------\n",
      "Input sentence: It doesn't hurt at all.\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah ríes extremo parezca romance romance viajar adoptado acto ícono impresora empiezo sé terrible enamorados reinó esperé aparentemente aprendería\n",
      "---------------------------------------------\n",
      "Input sentence: I'm going to be there.\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "Decoded sentence: [start] económico rampa regular crítico quén mastica escéptico recaudadas tocino transición coloca coloca enseña aceptar imaginas maletas colega escuchan edades doblar\n",
      "---------------------------------------------\n",
      "Input sentence: Tom seems to believe everything Mary says.\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah talaron digas digas perfil flores centimo costaron aventuras mataban abrirme kaoru aconsejo tenso rasguño caminando vistazo ausentes tomó\n",
      "---------------------------------------------\n",
      "Input sentence: I do not live to eat, but eat to live.\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 474ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "Decoded sentence: [start] ¡está contraría tesorero susto pie vecindario limpio notó fregar infectó flash callados rebeló separó llevarme ¡prepárate record ti arrastrado remar\n",
      "---------------------------------------------\n",
      "Input sentence: Mary is dressed in a skimpy outfit.\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "Decoded sentence: [start] olvidarlo decides subes peleábamos produciendo gps acompañaré acompañaré moderar míos retrasará retrasará arriesgues quebré individual pedírmelo las retires artículo campaña\n",
      "---------------------------------------------\n",
      "Input sentence: Tom told me to do whatever I want to do.\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Decoded sentence: [start] terminamos ¡ah talaron digas digas perfil piel salen neumáticos cogerá casi seguridad tokyo riesgo atrasado construyendo carretero hacía lamentará viaja\n",
      "---------------------------------------------\n",
      "Input sentence: My mother took my temperature.\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "Decoded sentence: [start] ¡está contraría tesorero civilización guerras cruzaron retó temía encuentros confusos relajarme tíralo preocuparme arreglo contador cuando cruce sordas sordas libras\n",
      "---------------------------------------------\n",
      "Input sentence: He has a cold.\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "Decoded sentence: [start] económico patada zapatillas definitiva desempleo profundas peluquero diez confiaban sociales presto limpiaría perdóname geniales llevarse habitación cogido arruinar comemos conductores\n",
      "---------------------------------------------\n",
      "Input sentence: I thought we had a deal.\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "Decoded sentence: [start] marchado ¡hemos vivamos ahuyentó resolvimos liberados fregadero fregadero dependiente irak sugerirle bolígrafo aburriendo equipo ¡Ése agresiva recetas personas encantada invencible\n",
      "---------------------------------------------\n",
      "Input sentence: We should have told him the truth.\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "Decoded sentence: [start] unido tenido sollozos trataban criminales sabiendo halcón quedan cables tildaron entusiasta venganza confíe bien rincón vives pinturas atribuye puerta suspendas\n",
      "---------------------------------------------\n",
      "Input sentence: Japan today is not what it was even ten years ago.\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Decoded sentence: [start] unido tenido sollozos calambre cinturón renunciar desapareció baseball españoles corro cubría colón colón llevábamos canadienses viviendo sufre tomes estribos vulnerables\n",
      "---------------------------------------------\n",
      "Input sentence: The question is whether he can do it or not.\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "Decoded sentence: [start] ¡mantente saltara manera trébol trataste trabajaré bienestar densa francia juro callados puramente racimo regresaré revoluciones desayunas salvaron miedo intentaría ajo\n",
      "---------------------------------------------\n",
      "Input sentence: Tom wasn't hurt in the accident.\n",
      "1/1 [==============================] - 0s 486ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "Decoded sentence: [start] hacelo acercaba irak ¡genial paquete sé sé sugeriste conferencia tocas existir playa desarrollo reconoce parecía mes roble guardaré ordenaron salvarle\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import load_model  \n",
    "\n",
    "# 데이터 준비\n",
    "spa_vocab = target_vectorization.get_vocabulary() # 스페인어 단어 집합\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab)) # 인덱스 단어 매핑\n",
    "max_decoded_sentence_length = 20 # 최대 길이\n",
    "\n",
    "# 문장 번역 함수\n",
    "# 20번동안 문장 해석\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence]) # 입력된 영어 문장 토큰화\n",
    "    decoded_sentence = '[start]' # 시작 토큰\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence]) # 현재까지 생성된 스페인어 문장의 토큰\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        # 입력된 영어 문장과 현재까지 생성된 스페인어 문장을 통해 다음 토큰 예측\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        # 예측 결과 중 가장 확률이 높은 토큰(인덱스를 처리해준다.)\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        # 선택된 토큰을 실제 스페인어로 변환\n",
    "        decoded_sentence += ' ' + sampled_token\n",
    "        # 공백을 붙여 출력해주겠다.\n",
    "        # 해석 중간에 종료 토큰을 만나면 break\n",
    "        if sampled_token == '[end]': # 종료 토큰\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence # 번역된 문장 반환\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-'*45)\n",
    "    print(\"Input sentence:\", input_sentence)\n",
    "    print(\"Decoded sentence:\", decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5dccdbb-0839-427d-96fd-200ac7be259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머를 사용한 Seq2Seq\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# TransformerDecoder 클래스 수정\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self._compute_causal_mask(inputs)\n",
    "        padding_mask = tf.cast(mask, dtype=tf.float32)[:, tf.newaxis, :] if mask is not None else None\n",
    "        combined_mask = tf.minimum(padding_mask, causal_mask) if padding_mask is not None else causal_mask\n",
    "\n",
    "        attn_output_1 = self.attention_1(inputs, inputs, attention_mask=combined_mask)\n",
    "        out_1 = self.layernorm_1(inputs + attn_output_1)\n",
    "\n",
    "        attn_output_2 = self.attention_2(out_1, encoder_outputs, attention_mask=padding_mask)\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def _compute_causal_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(seq_length)[:, tf.newaxis]\n",
    "        j = tf.range(seq_length)\n",
    "        mask = i >= j\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask = tf.reshape(mask, (1, seq_length, seq_length))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "365d1393-8acf-4058-9545-57ce2cb400cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 정보를 포함한 위치 인베딩층 작업\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "248108ac-81bc-42a7-87e8-539c13d785f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포터 인코더 제작\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='relu'),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attn_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        out_1 = self.layernorm_1(inputs + attn_output)\n",
    "        proj_output = self.dense_proj(out_1)\n",
    "        return self.layernorm_2(out_1 + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e745c745-d75e-41b6-a3b3-cdfde6cdfe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " english (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " spanish (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " positional_embedding_9 (Po  (None, None, 256)            3845120   ['english[0][0]']             \n",
      " sitionalEmbedding)                                                                               \n",
      "                                                                                                  \n",
      " positional_embedding_10 (P  (None, None, 256)            3845120   ['spanish[0][0]']             \n",
      " ositionalEmbedding)                                                                              \n",
      "                                                                                                  \n",
      " transformer_encoder_7 (Tra  (None, None, 256)            3155456   ['positional_embedding_9[0][0]\n",
      " nsformerEncoder)                                                   ']                            \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Tra  (None, None, 256)            5259520   ['positional_embedding_10[0][0\n",
      " nsformerDecoder)                                                   ]',                           \n",
      "                                                                     'transformer_encoder_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, None, 256)            0         ['transformer_decoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, None, 15000)          3855000   ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19960216 (76.14 MB)\n",
      "Trainable params: 19960216 (76.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "sequence_length = 20\n",
    "vocab_size = 15000\n",
    "\n",
    "# 입력 정의\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "\n",
    "# 인코더 정의\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# 디코더 정의\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력 레이어 정의\n",
    "decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "# 모델 정의\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 모델 요약\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3636b8b8-c86d-448c-96a8-63e08fe0d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  80/1302 [>.............................] - ETA: 1:37:47 - loss: 2.8665 - accuracy: 0.6653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59fa7e76-b332-4806-a8d4-f037650054d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "I never for a moment imagined that I would win.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Tom emptied the water out of the bottle before he refilled it with fresh water.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "She's going to sit on the yellow couch.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Come again.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Let's play basketball after school.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Despite all his wealth, he is stingy.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Where do you know each other from?\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "There was a castle here many years ago.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Where does the airport bus leave from?\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "I hate her parents.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Tom wanted Mary to leave right away.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "How long does it take?\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "What a beautiful ring!\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "I know what Tom is talking about.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Shoes are stiff when they are new.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Shoes are stiff when they are new.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "He's a friend of my brother's.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Tom is extremely clumsy.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "Tom got hit by a golf ball.\n",
      "[start] tom tom no [end]\n",
      "---------------------------------------------\n",
      "What forms do we need to file?\n",
      "[start] tom tom no [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "# vocabulary 설정 및 인덱스 조회\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_token\n",
    "        if sampled_token == '[end]':\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "# 테스트 쌍에서 영어 텍스트 가져오기\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-' * 45)\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8af260-3d97-4ba7-b229-b441cc0c09bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
