{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c49b53-c7ef-4e59-bdc2-2ccce4641951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화((텍스트를 벡터로 변환)\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower() # 텍스트를 소문자로 변환한다.\n",
    "        return \"\".join(char for char in text if char not in string.punctuation) # 구두점을 제거한다.\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split() # 공백을 기준으로 토큰화\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary={\"\":0, \"[UNK]\":1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text) # 표준화\n",
    "            tokens = self.tokenize(text) # 토큰화\n",
    "            for token in tokens: \n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "                    # 숫자와 value를 뒤바꾼 형태로 사전 제작('숫자-단어-숫자-단어' 형태로)\n",
    "\n",
    "        self.inverse_vocabulary = dict( # 텍스트를 정수 형태로\n",
    "            (v, k) for k, v in self.vocabulary.items()) \n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5990bd29-7f6c-43dc-9ba9-5a66c0182e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '[UNK]': 1, 'i': 2, 'write': 3, 'erase': 4, 'rewrite': 5, 'again': 6, 'and': 7, 'then': 8, 'a': 9, 'poppy': 10, 'blooms': 11}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms.',\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "print(vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c89c60-cdfc-42fe-b886-7a91847127dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a409576-158c-4c52-acc4-9e13194c6274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3654acf5-aabd-41fa-b4d9-b678f4678d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6159dda1-39ab-4665-ba38-74a09d2cee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standard_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f'[{re.escape(string.punctuation)}]', '')\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ae54ed-67ae-49c0-a304-fe2c77db0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=custom_standard_fn,  # 'standard' 대신 'standardize'\n",
    "    split=custom_split_fn  # 'split'은 올바르게 사용됨\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "247bfeea-3d2a-4a1b-a7b0-fd7c18b1cada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.adapt(dataset)\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd489a6a-4d68-4d07-afd3-ba0ec53f778b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc526bd5-88c8-4674-9fa2-e7c3356102ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "drcoded_sentence = ' '.join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48a37091-9439-4f61-a669-dcabc4b66ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open('aclImdb_v1.tar.gz', 'wb') as file:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79366e09-756e-4651-ae3f-4a515edc5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "340708a6-64cd-4a79-a922-c4e13b3785d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('aclImdb/train/unsup', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f698bba-374c-4824-8751-0a50d12c5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
     ]
    }
   ],
   "source": [
    "file_path = 'aclImdb/train/pos/4077_10.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20ef9581-0f91-4d2b-9875-b8010d3de87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "base_dir = pathlib.Path('aclImdb')\n",
    "val_dir = base_dir / 'val'\n",
    "train_dir = base_dir / 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2452ac94-9987-4b27-8009-92992223d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ('neg', 'pos'):\n",
    "    (val_dir/category).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(42).shuffle(files)\n",
    "\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir/category/fname, val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fd88030-2166-42d4-9638-c143dca4ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size)\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size)\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a14a21d-68a6-44b5-8597-000c0cfab57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape :  (32,)\n",
      "inputs.dtype :  <dtype: 'string'>\n",
      "targets.shape :  (32,)\n",
      "targets.dtype :  <dtype: 'int32'>\n",
      "inputs[0] :  tf.Tensor(b'Some films manage to survive almost on originality alone - \"Wonderland\" is certainly one of those films. The script manages to throw everything into a near-fever pitch, but without making it incoherent. The speed of this thriller is not to chosen to cover up a weak script, but rather to accurately reflect the drug-addled reality.<br /><br />As director, James Cox as a very peculiar way of working his actors. Most of the characters are perpetually on edge, and often because they\\'re rather quite ugly personalities. Val Kilmer has described John Holmes to be a hustler, able to manipulate and control. No offense to Kilmer, but his version of Holmes seems only able to control the drastically weak-minded. Nonetheless, it\\'s a stunning performance. Comparing this to Kilmer\\'s more \\'Hollywood\\' roles like in \"The Saint\" it seems to prove he is far more at home in gritty indie flicks.<br /><br />The actors are the main force holding all together. There are various little performances that stand out - especially the women. Carrie Fisher, Kate Bosworth, and Lisa Kudrow all have limited screen time next to their male counterparts, but they are all fantastic. Aside from Kilmer, Ted Levine and Dylan McDermott give a weird, stunning energy to their roles.<br /><br />I originally put off watching \"Wonderland\" because I assumed it was a film about a porn actor, in the strictest sense. Yes, the story revolves around John Holmes, but it has literally nothing to do with his professional career. Basically, this film is a murder mystery, and as such - it\\'s excellent.<br /><br />RATING: 7.5 out of 10', shape=(), dtype=string)\n",
      "targets[0] :  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print('inputs.shape : ', inputs.shape)\n",
    "    print('inputs.dtype : ', inputs.dtype)\n",
    "    print('targets.shape : ', targets.shape)\n",
    "    print('targets.dtype : ', targets.dtype)\n",
    "    print('inputs[0] : ', inputs[0])\n",
    "    print('targets[0] : ', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634a295-6a80-4d8c-84f1-cee6cd213846",
   "metadata": {},
   "source": [
    "# Bag of Words(Bow)\n",
    ": 구조나 순서를 무시하고 단어의 빈도에 초점을 맞춰서 텍스트 데이터를 수치화하여 모델 입력을 할 수 있도록 하는 기법\n",
    "- 단어 집합(Vocabulary) <-> 단어 빈도(Word Frequency) ==> 텍스트 데이터를 벡터로 표현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bf71156-e7eb-455b-9b5b-1a58cb353c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = 20000, # 최대 20,000개의 단어\n",
    "    output_mode = 'multi_hot', # 벡터화 방식 multi-hot encoding\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y : x) # 훈련 데이터에서 입력 데이터(x)만 추출\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fef25bb-9f92-4106-8579-75b105f88d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lgram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), # 입력 데이터 벡터화\n",
    "    num_parallel_calls=4 # CPU 코어\n",
    ")\n",
    "\n",
    "binary_lgram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), # 입력 데이터 벡터화\n",
    "    num_parallel_calls=4 # CPU 코어\n",
    ")\n",
    "\n",
    "binary_lgram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y), # 입력 데이터 벡터화\n",
    "    num_parallel_calls=4 # CPU 코어\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b41407d-59ca-420f-8fd1-dd216b3139fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape :  (32, 20000)\n",
      "inputs.dtype :  <dtype: 'float32'>\n",
      "targets.shape :  (32,)\n",
      "targets.dtype :  <dtype: 'int32'>\n",
      "inputs[0] :  tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0] :  tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_lgram_train_ds:\n",
    "    print('inputs.shape : ', inputs.shape)\n",
    "    print('inputs.dtype : ', inputs.dtype)\n",
    "    print('targets.shape : ', targets.shape)\n",
    "    print('targets.dtype : ', targets.dtype)\n",
    "    print('inputs[0] : ', inputs[0])\n",
    "    print('targets[0] : ', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86cb7f18-a29c-4358-ae48-f3026e635fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8ee8290-289e-4d53-8a73-b079c83080c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55205fb7-331e-47ac-96b0-f4debcf160c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4049 - accuracy: 0.8318 - val_loss: 0.2888 - val_accuracy: 0.8852\n",
      "Epoch 2/10\n",
      " 53/625 [=>............................] - ETA: 1s - loss: 0.2949 - accuracy: 0.8833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2762 - accuracy: 0.8947 - val_loss: 0.2691 - val_accuracy: 0.8968\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9132 - val_loss: 0.2754 - val_accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2342 - accuracy: 0.9208 - val_loss: 0.2936 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2194 - accuracy: 0.9265 - val_loss: 0.2996 - val_accuracy: 0.8924\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2142 - accuracy: 0.9304 - val_loss: 0.3093 - val_accuracy: 0.8918\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2194 - accuracy: 0.9298 - val_loss: 0.3154 - val_accuracy: 0.8940\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.2063 - accuracy: 0.9323 - val_loss: 0.3265 - val_accuracy: 0.8964\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.2119 - accuracy: 0.9338 - val_loss: 0.3332 - val_accuracy: 0.8876\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2104 - accuracy: 0.9338 - val_loss: 0.3378 - val_accuracy: 0.8890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x280fd8bb0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_lgram.h5', save_best_only=True)\n",
    "]\n",
    "# 데이터셋 캐싱 : 데이터 셋을 캐싱하면 디스크 I/O를 줄이고, 학습 속도를 높일 수 있다.\n",
    "# 데이터를 메모리에 저장하여 epoch가 빠르게 접근할 수 있도록 처리한다.\n",
    "model.fit(binary_lgram_train_ds.cache(), validation_data=binary_lgram_val_ds.cache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d65c0304-ba8a-4f22-927c-e00cd43ab503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2876 - accuracy: 0.8888\n",
      "테스트 정확도 : 0.889\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('binary_lgram.h5')\n",
    "print(f'테스트 정확도 : {model.evaluate(binary_lgram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6143d48-59e2-4ca0-98ad-23205b17d8d1",
   "metadata": {},
   "source": [
    "## 바이그램(Bigram)\n",
    ": 연속된 두 단어 쌍\n",
    "\n",
    "#### * 이진 인코딩 : 바이그램이 존재하면 1, 존재하지 않으면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87813601-1ac6-4d7f-8152-1ac73d9a1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='multi_hot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1cf55-892d-431f-9535-f9b247ac93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y : (text_vectorization(x), y),\n",
    "    num_paralled_calls = 4)\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y : (text_vectorization(x), y),\n",
    "    num_paralled_calls = 4)\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y : (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa1e5c-0eb3-4b46-b558-12b1d2a859a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe2886-1080-4d5f-b931-259d27e13ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data = binary_2gram_cal_ds.chache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd4fb2-6d99-4d75-8c22-20019d2be935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('binary_2gram.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e36d4-43d2-400b-9471-910f476e1841",
   "metadata": {},
   "source": [
    "# TF-IDF 벡터라이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "067091a5-27ce-4517-95ac-94526ea6ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 카운트 반환\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='count' # 단어의 반도기반 벡터화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94eb4e67-c036-4763-b6d0-440d3c1b93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 가중치가 적용된 출력 반환\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='tf_idf' # TF-IDF 기반 벡터화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec488722-ba5f-43db-9c6b-d5f205317d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y : (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y, : (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y, : (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7162579a-9652-4477-82ee-370ce9ac156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.4821 - accuracy: 0.7746 - val_loss: 0.2993 - val_accuracy: 0.8862\n",
      "Epoch 2/10\n",
      " 66/625 [==>...........................] - ETA: 1s - loss: 0.3537 - accuracy: 0.8546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8485 - val_loss: 0.3171 - val_accuracy: 0.8802\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.8603 - val_loss: 0.3227 - val_accuracy: 0.8772\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3093 - accuracy: 0.8732 - val_loss: 0.3224 - val_accuracy: 0.8738\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2896 - accuracy: 0.8812 - val_loss: 0.3471 - val_accuracy: 0.8566\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.8840 - val_loss: 0.3529 - val_accuracy: 0.8704\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2715 - accuracy: 0.8874 - val_loss: 0.3369 - val_accuracy: 0.8720\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2642 - accuracy: 0.8887 - val_loss: 0.3554 - val_accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2723 - accuracy: 0.8907 - val_loss: 0.3511 - val_accuracy: 0.8618\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2553 - accuracy: 0.8985 - val_loss: 0.3549 - val_accuracy: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x282933af0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('tfidf_2gram.h5', save_best_only=True)\n",
    "\n",
    "]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(), validation_data = tfidf_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aeac1dd6-0466-4850-9318-5034b0c3b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2977 - accuracy: 0.8910\n",
      "테스트 정확도 : 0.891\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('tfidf_2gram.h5')\n",
    "print(f'테스트 정확도 : {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9699350c-4543-47eb-b41c-a2367f4ddcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9222ef67-d614-4df3-b8b8-7a82f5811ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적 리뷰일 확률 : 67.41 %\n"
     ]
    }
   ],
   "source": [
    "raw_text_data = tf.convert_to_tensor([['That was an excellent moive, I love it.']])\n",
    "\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f'긍정적 리뷰일 확률 : {float(predictions[0] * 100):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62266a-4025-4c44-b14b-abfb6ab1f655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
