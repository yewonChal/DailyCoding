{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20202164-51ce-47a8-aa56-252a34eeca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc63b446-8fc4-41f0-b020-542b160e741b",
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open('fra.txt', 'r', encoding='utf-8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            src_line, tar_line, _ = line.strip().split('\\t') # source, target 가져오기\n",
    "\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()] # 단어 기준\n",
    "\n",
    "            tar_line = preprocessing_sentence(tar_line) # 문장 기준\n",
    "            tar_line_in = [w for w in ('<sos> ' + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + ' <eos>').split()]\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == 59999:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc4c9fc6-ce41-495d-a22e-f871f373ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open('fra.txt', 'r', encoding='utf-8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')  # source, target 가져오기\n",
    "\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]  # 단어 기준\n",
    "\n",
    "            tar_line = preprocess_sentence(tar_line)  # 문장 기준\n",
    "            tar_line_in = [w for w in ('<sos> ' + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + ' <eos>').split()]\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == 59999:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef394ff-3801-45e3-909e-43d60612c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # 프랑스어 악센트 삭제\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5040726-c02b-4eff-9032-6dad2eacc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "    sent = re.sub(r'([?.!,¿])', r' \\1', sent) # 구두점이 나오면 공백 추가\n",
    "    sent = re.sub(r'[^a-zA-Z!.?]+', r' ', sent) # 알파벳, !, ., ? 제외하고는 전부 삭제\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # 공백이 여러개 나오면 하나의 공백으로 변환\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb11559b-0c28-42f8-b863-31dba0f852c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 :  Have you had dinner?\n",
      "전처리 후 영어 문장 :  have you had dinner ?\n",
      "전처리 전 불어 문장 :  Avez-vous déjà diné?\n",
      "전처리 후 불어 문장 :  avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 : ', en_sent)\n",
    "print('전처리 후 영어 문장 : ', preprocess_sentence(en_sent))\n",
    "print('전처리 전 불어 문장 : ', fr_sent)\n",
    "print('전처리 후 불어 문장 : ', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9865910-70ef-4bce-928b-0787787bc603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 :  Have you had dinner?\n",
      "전처리 후 영어 문장 :  have you had dinner ?\n",
      "전처리 전 불어 문장 :  Avez-vous déjà diné?\n",
      "전처리 후 불어 문장 :  avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 : ', en_sent)\n",
    "print('전처리 후 영어 문장 : ', preprocess_sentence(en_sent))\n",
    "print('전처리 전 불어 문장 : ', fr_sent)\n",
    "print('전처리 후 불어 문장 : ', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe4b33c8-dfad-46b3-b957-f45b7ab75ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6bd6855-f8c6-49c4-9241-deca4f012500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 :  [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 :  [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 :  [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력 : ', sents_en_in[:5])\n",
    "print('디코더의 입력 : ', sents_fra_in[:5])\n",
    "print('디코더의 레이블 : ', sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "052d69c3-9a8b-4657-bf89-0835f36a5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters='', lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding='post')\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters='', lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding='post')\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69e289e9-7d94-44f4-9fab-9eaeb09131d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 8) (60000, 17) (60000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input.shape, decoder_input.shape, decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e869772f-c5f6-4998-9725-70cbf452e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d5e97e1-6b7b-43ba-9e8b-bcca70dbd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 17\n"
     ]
    }
   ],
   "source": [
    "max_src_len = encoder_input.shape[1]\n",
    "max_tar_len = decoder_input.shape[1]\n",
    "\n",
    "print(max_src_len, max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47c4659b-330d-47b1-b3e8-5daec2b28a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6443 10934\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index)+1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index)+1\n",
    "\n",
    "print(src_vocab_size, tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76039260-95c2-4a15-834a-16f30ddd333e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [ 32,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [ 32,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [ 32,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [984,   1,   0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b83188c8-515b-49fe-937a-145748031e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44980 15862 35909 ... 31098 38134 43181]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbb76f7e-a4d6-43b0-9bde-a2b91fb1ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e0ae95f-b5be-4849-9d8e-2e16c7f6e562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6,  171,   10, 1964,    1,    0,    0,    0],\n",
       "       [   2,   78,  222,    1,    0,    0,    0,    0],\n",
       "       [4218,   69, 1023,    1,    0,    0,    0,    0],\n",
       "       [   3,   12,  190,    1,    0,    0,    0,    0],\n",
       "       [  43,   19,  264,   26,  881,    1,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3190dfe8-be10-4003-bfaf-207413825008",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = 6000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a4c254e-2179-45d3-9847-7f36a86d0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 8) (54000, 17) (54000, 17)\n",
      "(6000, 8) (6000, 17) (6000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape, decoder_input_train.shape, decoder_target_train.shape)\n",
    "print(encoder_input_test.shape, decoder_input_test.shape, decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "742d2163-4821-4f93-9c09-a5a6af92b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "370d3c3e-c467-45ed-9fbc-1354cc0574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a8cd077-2960-43dd-b699-1887a261be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None, ))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # padding 0을 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) \n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "356ce70b-2a59-4ebb-a25d-ab2d646a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# 인코더의 상태를 디코더의 초기 상태로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점에 대한 결과를 소프트 맥스 함수로 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "181374a0-3a65-4e3c-a636-96a7c9e1885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12c65f5a-ed0d-4149-b39f-46300ec95ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26fc4a81-acea-4e8b-9261-219f02c042cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 128)    824704      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 64)     699776      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " masking_2 (Masking)            (None, None, 128)    0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " masking_3 (Masking)            (None, None, 64)     0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 64),         49408       ['masking_2[0][0]']              \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 64),   33024       ['masking_3[0][0]',              \n",
      "                                 (None, 64),                      'lstm_2[0][1]',                 \n",
      "                                 (None, 64)]                      'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 10934)  710710      ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,317,622\n",
      "Trainable params: 2,317,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "042d85e7-b9da-4b54-bda4-93176199f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a66334a8-3caa-42c9-bcf6-5872bb51bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "422/422 [==============================] - 233s 543ms/step - loss: 2.8615 - acc: 0.6224 - val_loss: 1.8651 - val_acc: 0.6893\n",
      "Epoch 2/50\n",
      "422/422 [==============================] - 215s 509ms/step - loss: 1.7042 - acc: 0.7310 - val_loss: 1.5660 - val_acc: 0.7514\n",
      "Epoch 3/50\n",
      "422/422 [==============================] - 210s 498ms/step - loss: 1.4748 - acc: 0.7607 - val_loss: 1.3998 - val_acc: 0.7769\n",
      "Epoch 4/50\n",
      "422/422 [==============================] - 213s 505ms/step - loss: 1.3198 - acc: 0.7888 - val_loss: 1.2789 - val_acc: 0.7961\n",
      "Epoch 5/50\n",
      "422/422 [==============================] - 229s 544ms/step - loss: 1.2081 - acc: 0.8050 - val_loss: 1.1896 - val_acc: 0.8107\n",
      "Epoch 6/50\n",
      "422/422 [==============================] - 202s 479ms/step - loss: 1.1171 - acc: 0.8166 - val_loss: 1.1182 - val_acc: 0.8184\n",
      "Epoch 7/50\n",
      "422/422 [==============================] - 163s 385ms/step - loss: 1.0382 - acc: 0.8260 - val_loss: 1.0560 - val_acc: 0.8265\n",
      "Epoch 8/50\n",
      "422/422 [==============================] - 161s 381ms/step - loss: 0.9678 - acc: 0.8342 - val_loss: 1.0019 - val_acc: 0.8330\n",
      "Epoch 9/50\n",
      "422/422 [==============================] - 121s 287ms/step - loss: 0.9047 - acc: 0.8410 - val_loss: 0.9550 - val_acc: 0.8373\n",
      "Epoch 10/50\n",
      "422/422 [==============================] - 121s 286ms/step - loss: 0.8491 - acc: 0.8466 - val_loss: 0.9160 - val_acc: 0.8412\n",
      "Epoch 11/50\n",
      "422/422 [==============================] - 122s 288ms/step - loss: 0.8000 - acc: 0.8518 - val_loss: 0.8814 - val_acc: 0.8443\n",
      "Epoch 12/50\n",
      "422/422 [==============================] - 121s 288ms/step - loss: 0.7556 - acc: 0.8568 - val_loss: 0.8537 - val_acc: 0.8485\n",
      "Epoch 13/50\n",
      "422/422 [==============================] - 145s 344ms/step - loss: 0.7158 - acc: 0.8615 - val_loss: 0.8284 - val_acc: 0.8515\n",
      "Epoch 14/50\n",
      "422/422 [==============================] - 228s 540ms/step - loss: 0.6790 - acc: 0.8661 - val_loss: 0.8068 - val_acc: 0.8537\n",
      "Epoch 15/50\n",
      "422/422 [==============================] - 230s 546ms/step - loss: 0.6454 - acc: 0.8702 - val_loss: 0.7860 - val_acc: 0.8564\n",
      "Epoch 16/50\n",
      "422/422 [==============================] - 229s 543ms/step - loss: 0.6142 - acc: 0.8742 - val_loss: 0.7700 - val_acc: 0.8584\n",
      "Epoch 17/50\n",
      "422/422 [==============================] - 222s 526ms/step - loss: 0.5850 - acc: 0.8780 - val_loss: 0.7520 - val_acc: 0.8605\n",
      "Epoch 18/50\n",
      "422/422 [==============================] - 207s 491ms/step - loss: 0.5577 - acc: 0.8819 - val_loss: 0.7406 - val_acc: 0.8628\n",
      "Epoch 19/50\n",
      "422/422 [==============================] - 232s 551ms/step - loss: 0.5319 - acc: 0.8857 - val_loss: 0.7270 - val_acc: 0.8649\n",
      "Epoch 20/50\n",
      "422/422 [==============================] - 208s 493ms/step - loss: 0.5081 - acc: 0.8893 - val_loss: 0.7162 - val_acc: 0.8661\n",
      "Epoch 21/50\n",
      "422/422 [==============================] - 183s 434ms/step - loss: 0.4857 - acc: 0.8926 - val_loss: 0.7062 - val_acc: 0.8674\n",
      "Epoch 22/50\n",
      "422/422 [==============================] - 201s 476ms/step - loss: 0.4651 - acc: 0.8962 - val_loss: 0.6986 - val_acc: 0.8689\n",
      "Epoch 23/50\n",
      "422/422 [==============================] - 204s 484ms/step - loss: 0.4452 - acc: 0.8995 - val_loss: 0.6890 - val_acc: 0.8706\n",
      "Epoch 24/50\n",
      "422/422 [==============================] - 227s 539ms/step - loss: 0.4268 - acc: 0.9026 - val_loss: 0.6820 - val_acc: 0.8721\n",
      "Epoch 25/50\n",
      "422/422 [==============================] - 199s 471ms/step - loss: 0.4095 - acc: 0.9054 - val_loss: 0.6760 - val_acc: 0.8730\n",
      "Epoch 26/50\n",
      "422/422 [==============================] - 197s 467ms/step - loss: 0.3934 - acc: 0.9086 - val_loss: 0.6722 - val_acc: 0.8740\n",
      "Epoch 27/50\n",
      "422/422 [==============================] - 167s 395ms/step - loss: 0.3783 - acc: 0.9113 - val_loss: 0.6674 - val_acc: 0.8754\n",
      "Epoch 28/50\n",
      "422/422 [==============================] - 125s 295ms/step - loss: 0.3641 - acc: 0.9139 - val_loss: 0.6619 - val_acc: 0.8758\n",
      "Epoch 29/50\n",
      "422/422 [==============================] - 125s 295ms/step - loss: 0.3507 - acc: 0.9162 - val_loss: 0.6591 - val_acc: 0.8764\n",
      "Epoch 30/50\n",
      "422/422 [==============================] - 123s 290ms/step - loss: 0.3386 - acc: 0.9185 - val_loss: 0.6540 - val_acc: 0.8778\n",
      "Epoch 31/50\n",
      "422/422 [==============================] - 123s 292ms/step - loss: 0.3269 - acc: 0.9209 - val_loss: 0.6536 - val_acc: 0.8779\n",
      "Epoch 32/50\n",
      "422/422 [==============================] - 124s 293ms/step - loss: 0.3160 - acc: 0.9228 - val_loss: 0.6518 - val_acc: 0.8782\n",
      "Epoch 33/50\n",
      "422/422 [==============================] - 201s 478ms/step - loss: 0.3058 - acc: 0.9250 - val_loss: 0.6496 - val_acc: 0.8795\n",
      "Epoch 34/50\n",
      "422/422 [==============================] - 196s 465ms/step - loss: 0.2961 - acc: 0.9268 - val_loss: 0.6482 - val_acc: 0.8796\n",
      "Epoch 35/50\n",
      "422/422 [==============================] - 195s 462ms/step - loss: 0.2871 - acc: 0.9284 - val_loss: 0.6490 - val_acc: 0.8794\n",
      "Epoch 36/50\n",
      "422/422 [==============================] - 204s 484ms/step - loss: 0.2785 - acc: 0.9302 - val_loss: 0.6475 - val_acc: 0.8803\n",
      "Epoch 37/50\n",
      "422/422 [==============================] - 195s 463ms/step - loss: 0.2708 - acc: 0.9318 - val_loss: 0.6460 - val_acc: 0.8807\n",
      "Epoch 38/50\n",
      "422/422 [==============================] - 200s 473ms/step - loss: 0.2629 - acc: 0.9331 - val_loss: 0.6459 - val_acc: 0.8810\n",
      "Epoch 39/50\n",
      "422/422 [==============================] - 197s 468ms/step - loss: 0.2558 - acc: 0.9348 - val_loss: 0.6444 - val_acc: 0.8815\n",
      "Epoch 40/50\n",
      "422/422 [==============================] - 197s 468ms/step - loss: 0.2489 - acc: 0.9360 - val_loss: 0.6456 - val_acc: 0.8814\n",
      "Epoch 41/50\n",
      "422/422 [==============================] - 206s 488ms/step - loss: 0.2425 - acc: 0.9371 - val_loss: 0.6472 - val_acc: 0.8817\n",
      "Epoch 42/50\n",
      "422/422 [==============================] - 211s 501ms/step - loss: 0.2366 - acc: 0.9382 - val_loss: 0.6483 - val_acc: 0.8814\n",
      "Epoch 43/50\n",
      "422/422 [==============================] - 203s 481ms/step - loss: 0.2308 - acc: 0.9394 - val_loss: 0.6488 - val_acc: 0.8818\n",
      "Epoch 43: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16109b130>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, batch_size=128, epochs=50, \n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test) , callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1b3af90-8a8f-4010-a248-1d0220a1b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a492a2dc-f377-41b3-828e-a3fe1128a158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 16:38:50.013309: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-15 16:38:50.013945: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-15 16:38:50.014799: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_state2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eab7fa1d-9b9d-43a5-9cb8-2af38fa3214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94a7c75e-5ad6-44c2-baad-63bac533a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq) # 인코더 모델이기 때문에 인코더 상태를 반환받는다\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성(문장의 시작)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value) # 이전 시점의 상태를 현 시점의 초기 상태\n",
    "\n",
    "        # 가장 높은 확률의 단어 찾기\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 76):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현 시점의 예측을 다음 시점의 입력으로 사용\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현 시점의 상태를 다음 시점의 초기 상태로 사용\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2b18fa2-5fd8-4fa3-84fc-2b98fe049a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_src(input_seq): # 정수 데이터를 문자 데이터로\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word!=0):\n",
    "            sentence += index_to_src[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33ad6c87-4a81-44b7-9548-373f4300c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence += index_to_tar[encoded_word] + ' '\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a1b04a9-d0fe-4691-925b-e74a8469e4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4151176579.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[66], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    print('정답 문장 : ', seq_to_tar(decoder_input_train[seq_index])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,2,3,4,5,6,7,8]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print('입력 문장 : ', seq_to_src(encoder_input_train[seq_index])\n",
    "    print('정답 문장 : ', seq_to_tar(decoder_input_train[seq_index])\n",
    "    print('번역 문장 : ', decoded_sentence[1:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8521cad7-cba5-49ad-8adf-8fda933d54b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2990389357.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[65], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    print('정답 문장 : ', seq_to_tar(decoder_input_test[seq_index])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,2,3,4,5,6,7,8]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print('입력 문장 : ', seq_to_src(encoder_input_test[seq_index])\n",
    "    print('정답 문장 : ', seq_to_tar(decoder_input_test[seq_index])\n",
    "    print('번역 문장 : ', decoded_sentence[1:-5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a0bf3c8-aefb-476a-a1ad-a10b372d1544",
   "metadata": {},
   "source": [
    "# BLEU(Bilingual Evaluation Understudy)\n",
    "### : 언어 모델 평가 방법\n",
    "\n",
    "1. n-gram 매칭 : 번역된 텍스트와 기존 텍스트 간의 n-gram 일치를 측정한다.(보통 1, 2, 3, 4 -gram으로 측정)\n",
    "2. 정밀도(Precision) : 일치하는 n-gram의 비율\n",
    "-> p(n) = Number of matching n-gram / Total number of n-grams in the translated text\n",
    "\n",
    "3. 길이 패널티(Brevity Penalty) : 텍스트가 너무 짧게 번역되지 않도록 패널티를 적용\n",
    "-> BP = 1          (if c > r)\n",
    "        e^(1-r/c)  (if c <= r)\n",
    "c는 번역 텍스트 길이, r은 기존 텍스트 길이\n",
    "\n",
    "4. 점수 합산 : n-gram의 정밀도를 기하평균으로 계산한다.(0~1의 점수로 나타내며 1에 가까울 수록 품질이 좋음을 나타낸다.)\n",
    "-> BLEU = BP * exp(∑ w(n) * log(p(n)))\n",
    "-> w(n)은 각 n-gram의 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "067e154f-0d79-4450-bdc5-ba0ddc6fb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94d068ed-4ccc-4ae8-b726-561f5daa26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_count(tokens, n): # 단순히 n-gram을 세주는 코드\n",
    "    return Counter(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "527e2591-9618-4eb2-bf3a-609b9a9827a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party.'\n",
    "tokens = candidate.split() # split 처리\n",
    "result = simple_count(tokens, 1) # split 처리 count\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e01c1173-b2fe-4d08-b98a-edcf883a926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 8})\n"
     ]
    }
   ],
   "source": [
    "candidate = 'the the the the the the the the'\n",
    "tokens = candidate.split()\n",
    "result = simple_count(tokens, 1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91eae9f7-ae0e-4064-9d46-4dcf81b24bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count clip\n",
    "def count_clip(candidate, reference_list, n): # n-gram의 등장횟수를 보정(clipping)하여 정확한 평가를 할 수 있게 돕는다.\n",
    "    ca_cnt = simple_count(candidate, n) # condidate : 번역된 문장, reference_list : 참조 문장들, n : n-gram\n",
    "    # condidate에서 n-gram\n",
    "    max_ref_cnt_dict = dict()\n",
    "\n",
    "    for ref in reference_list:\n",
    "        # 각 reference 문장에서 n-gram\n",
    "        ref_cnt = simple_count(ref, n)\n",
    "\n",
    "        # 각 reference 문장에서 n-gram의 최대 횟수를 계산\n",
    "        for n_gram in ref_cnt:\n",
    "            if n_gram in max_ref_cnt_dict:\n",
    "                max_ref_cnt_dict[n_gram] = max(ref_cnt[n_gram], max_ref_cnt_dict[n_gram])\n",
    "            else:\n",
    "                max_ref_cnt_dict[n_gram] = ref_cnt[n_gram]\n",
    "\n",
    "    return {n_gram : min(ca_cnt.get(n_gram, 0), max_ref_cnt_dict.get(n_gram, 0)) for n_gram in ca_cnt} # 참조문장의 횟수를 초과하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfeb58f8-1285-4e9c-ab14-b391cd21ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the',): 2}\n"
     ]
    }
   ],
   "source": [
    "references = ['the cat is on the mat', 'there is a cat on the mat']\n",
    "\n",
    "result = count_clip(candidate.split(), list(map(lambda ref : ref.split(), references)), 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bfe4729-6ebf-4a52-943c-a75a56bc5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate, reference_list, n):\n",
    "    # 분지\n",
    "    clip_cnt = count_clip(candidate, reference_list, n)\n",
    "    total_clip_cnt = sum(clip_cnt.values())\n",
    "\n",
    "    # 분모\n",
    "    cnt = simple_count(candidate, n)\n",
    "    total_cnt = sum(cnt.values())\n",
    "\n",
    "    if total_cnt == 0:\n",
    "        total_cnt = 1\n",
    "\n",
    "    return total_clip_cnt / total_cnt # count_clip의 합 / 단순 count 합  = 보정된 정밀도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14d31f70-4193-4dc6-b68c-4f0265197d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "result = modified_precision(candidate.split(), list(map(lambda ref : ref.split(), references)), n=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcc69bac-0e35-4178-934e-149c37e3ceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3610854011224141\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "reference = [['this', 'is', 'a', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1262c8a5-b1e7-4be1-b027-6790d328435a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1661692658.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[76], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    ref_lens = len(ref) for ref in reference_list) # 참조 문장들의 길이\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def closest_ref_length(candidate, reference_list):\n",
    "    ca_len = len(candidate) # 번역된 문장 길이\n",
    "    ref_lens = len(ref) for ref in reference_list) # 참조 문장들의 길이\n",
    "\n",
    "        closest_ref_len = min(ref_lens, key=lambda ref_len : (abs(ref_len - ca_len), ref_len))\n",
    "        return closest_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49589823-3575-4f56-af61-6acd6e863454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference_list):\n",
    "    ca_len = len(candidate)\n",
    "    ref_len = closest_ref_length(candidate, reference_list)\n",
    "\n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "\n",
    "    elif ca_len == 0:\n",
    "        return 0\n",
    "\n",
    "    else:\n",
    "        return np.exp(1-ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12ccb135-7122-4a84-a754-c276b0d51aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(candidate, reference_list)\n",
    "\n",
    "    p_n = [modified_precision(candidate, reference_list, n=n) for n, _ in enumerate(weights, start=1)] # 정밀도 계산\n",
    "    score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "\n",
    "    return bp*np.exp(score)\n",
    "\n",
    "# 4. 점수 합산 : n-gram의 정밀도를 기하평균으로 계산한다.\n",
    "# -> BLEU = BP * exp(∑ w(n) * log(p(n)))\n",
    "# -> w(n)은 각 n-gram의 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "493274f4-dbef-4faf-ad83-faf73769a6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'closest_ref_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is a guide to action which ensures that the military always obeys the commands of the party.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m references \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is a guide to action that ensures that the military will forever heed Party commands\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is the guiding principle which guarantees the military forces always being under the command of the Party\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is the practical guide for the army always to heed the directions of the party\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m실습 코드 BLEU : \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mbleu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLTK의 BLEU : \u001b[39m\u001b[38;5;124m'\u001b[39m, bleu\u001b[38;5;241m.\u001b[39msentence_bleu(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m ref : ref\u001b[38;5;241m.\u001b[39msplit(), references)), candidate,split()))\n",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m, in \u001b[0;36mbleu_score\u001b[0;34m(candidate, reference_list, weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbleu_score\u001b[39m(candidate, reference_list, weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     bp \u001b[38;5;241m=\u001b[39m \u001b[43mbrevity_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     p_n \u001b[38;5;241m=\u001b[39m [modified_precision(candidate, reference_list, n\u001b[38;5;241m=\u001b[39mn) \u001b[38;5;28;01mfor\u001b[39;00m n, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weights, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;66;03m# 정밀도 계산\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum([w_i \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(p_i) \u001b[38;5;28;01mif\u001b[39;00m p_i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w_i, p_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, p_n)])\n",
      "Cell \u001b[0;32mIn[77], line 3\u001b[0m, in \u001b[0;36mbrevity_penalty\u001b[0;34m(candidate, reference_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbrevity_penalty\u001b[39m(candidate, reference_list):\n\u001b[1;32m      2\u001b[0m     ca_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(candidate)\n\u001b[0;32m----> 3\u001b[0m     ref_len \u001b[38;5;241m=\u001b[39m \u001b[43mclosest_ref_length\u001b[49m(candidate, reference_list)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ca_len \u001b[38;5;241m>\u001b[39m ref_len:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'closest_ref_length' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party.'\n",
    "references = [\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party',\n",
    "    'It is the practical guide for the army always to heed the directions of the party'\n",
    "]\n",
    "\n",
    "print('실습 코드 BLEU : ', bleu_score(candidate.split(), list(map(lambda ref : ref.split(), references))))\n",
    "print('NLTK의 BLEU : ', bleu.sentence_bleu(list(map(lambda ref : ref.split(), references)), candidate,split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084318d-b09b-407f-b8f6-49871327199b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
