{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d422c545-9153-42b6-8a63-63bc4d294bc9",
   "metadata": {},
   "source": [
    "# konlpy(Korean Natural Language Processing in Python)란?\n",
    "## : 한국어 텍스트 데이터 처리(형태소 분석, 품사 태깅, 명사 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef12919-de0a-4517-85f6-a5257fac90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphs: ['성심당', '망고', '시루', '를', '먹어', '보고', '싶었는데', '타이밍', '을', '놓쳤어요', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = '성심당 망고시루를 먹어보고 싶었는데 타이밍을 놓쳤어요.'\n",
    "\n",
    "morphs = okt.morphs(text)\n",
    "print('Morphs:', morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbba8c7-1dbf-4a0d-ac62-b33b9c6e15c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphs: ['한글', '자연어', '처리', '가', '필요하다', '.', '하지만', '한국어', '는', '굉장히', '복잡한', '구조', '로', '이루어져', '있다', '.']\n",
      "POS: [('한글', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('가', 'Josa'), ('필요하다', 'Adjective'), ('.', 'Punctuation'), ('하지만', 'Conjunction'), ('한국어', 'Noun'), ('는', 'Josa'), ('굉장히', 'Adjective'), ('복잡한', 'Adjective'), ('구조', 'Noun'), ('로', 'Josa'), ('이루어져', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n",
      "Nouns: ['한글', '자연어', '처리', '한국어', '구조']\n",
      "Stemmed: ['한글', '자연어', '처리', '가', '필요하다', '.', '하지만', '한국어', '는', '굉장하다', '복잡하다', '구조', '로', '이루어지다', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = '한글 자연어 처리가 필요하다. 하지만 한국어는 굉장히 복잡한 구조로 이루어져 있다.'\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(text)\n",
    "print('Morphs:', morphs)\n",
    "\n",
    "# 품사태깅\n",
    "pos = okt.pos(text)\n",
    "print('POS:', pos)\n",
    "\n",
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "print('Nouns:', nouns)\n",
    "\n",
    "# 어간 추출\n",
    "stemmed = okt.morphs(text, stem=True)\n",
    "print('Stemmed:', stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18025f0-7195-4417-8382-74bf35bf286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = '<PAD>'\n",
    "SOS = '<SOS>'\n",
    "END = '<END>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "PAD_INDEX = 0\n",
    "DOD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, SOS, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "MAX_SEQUENCE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e160de-f4ae-4c92-8cfa-837cead7f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3086da80-4401-43e3-855d-1b3c7c30cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ChatBotData.csv_short'\n",
    "\n",
    "inputs, outputs = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfc55c8-bb3f-476e-8ee6-0e682c4b722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다'],\n",
       " ['그 사람도 그럴 거예요.',\n",
       "  '그 사람도 그럴 거예요.',\n",
       "  '혼자를 즐기세요.',\n",
       "  '돈은 다시 들어올 거예요.',\n",
       "  '땀을 식혀주세요.'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:5], outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44da498-c1f9-4db2-9283-5055aecab60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
    "    vocabulary_list = []\n",
    "\n",
    "    if not os.path.exists(vocab_path):\n",
    "        if os.path.exists(path) :\n",
    "            data_df = pd.read_csv(path, encoding='utf-8')\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if tokenize_as_morph:\n",
    "                question = prepro_like_morphlized(question)\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "\n",
    "            data = []\n",
    "\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "\n",
    "            words = data_tokenizer(data)\n",
    "            words = list(set(words))\n",
    "\n",
    "            words[:0] = MARKER\n",
    "\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "\n",
    "    return char2idx, idx2char, len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6459b258-2a7e-442f-a7a3-a629942b86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_like_morphlized(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = list()\n",
    "    for seq in tqdm(data):\n",
    "        morplized_seq = ' '.join(morph_analyzer.morphs(seq.replaxe(' ', '')))\n",
    "        result_data.append(morpglized_seq)\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c8cd9c-2839-4a14-b1ac-35df33ecf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(data):\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        sentence = re.sub(CHANGE_FILTER, '', sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d6ff06d-8d5f-449f-894a-127eaac19545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(vocabulary_list):\n",
    "    char2idx = {char:idx for idx, char in enumerate(vocabulary_list)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09aa5afc-88d9-4540-a304-14dbf9b3cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'vocabulary2.txt'\n",
    "\n",
    "char2idx, idx2char, vocab_size = load_vocabulary(path, vocab_path, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f5d9cf7-ebc3-4139-98c8-a4cfb9369d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<END>': 2, '<UNK>': 3, '질린다': 4, '달에는': 5, '궁금해': 6, '가상화폐': 7, '운동만': 8, '돌아가서': 9, '열': 10, '훈훈해': 11, '식혀주세요': 12, '비싼데': 13, '좋을': 14, '선물로': 15, '감기': 16, '새출발': 17, '절약해봐요': 18, '뭐가': 19, '어서': 20, '따뜻하게': 21, '좀': 22, '그럴': 23, '뭐하는지': 24, '보인다': 25, '같아요': 26, '생일인데': 27, '가난한': 28, '집에': 29, '뭘': 30, '마세요': 31, '더': 32, '다시': 33, '같아': 34, '걸리겠어': 35, '나왔다': 36, '땀을': 37, '설움': 38, '운동': 39, '해보세요': 40, '사세요': 41, '승진': 42, '생각해보세요': 43, '망함': 44, '때까지': 45, '너무': 46, '가끔은': 47, '가스비': 48, '적당히': 49, '가스불': 50, '설득해보세요': 51, '전생에': 52, '자의': 53, '나라를': 54, '필요한': 55, '가끔': 56, '돈은': 57, '집착하지': 58, '교회': 59, '나갔어': 60, '빨리': 61, '혼자인게': 62, '또': 63, '구하셨나요': 64, '싶어': 65, '안': 66, '땀난다': 67, '믿어줘': 68, '게': 69, '나': 70, '좋다': 71, '마음을': 72, '켜놓고': 73, '즐기세요': 74, '다음': 75, '켜고': 76, '거예요': 77, '따라': 78, '남자친구가': 79, '갔어': 80, '좋을까': 81, '운동을': 82, '남자친구': 83, '필요했던': 84, '그': 85, '해': 86, '잊고': 87, '나온거': 88, '바빠': 89, '혼자를': 90, '많이': 91, '나오세요': 92, '들어올': 93, '좋아요': 94, '결단은': 95, '오늘': 96, '줄까': 97, '쫄딱': 98, '있어도': 99, '가만': 100, '빠를수록': 101, '평소에': 102, '끄고': 103, '함께': 104, '사람도': 105, '하세요': 106, '것': 107, '잘생겼어': 108, '거짓말': 109, '데려가고': 110}\n"
     ]
    }
   ],
   "source": [
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b5590fb-ee52-4130-b553-3320de2fec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_input_index = [] # 인덱스 값\n",
    "    sequences_length = [] # 문장 길이\n",
    "\n",
    "    # 형태소 토크나이징\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, '', sequence)\n",
    "        sequence_index = []\n",
    "\n",
    "        # UNK\n",
    "        for word in sequence.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else :\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "\n",
    "        # 문장 길이 제한\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "            \n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # PAD\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "\n",
    "        #sequences_length.append(len(sequence_index))\n",
    "\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edd90cb1-bdde-4c43-af7c-aa74e89adcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inputs, inputs_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361d6b50-c13b-40ac-8a68-ad98723522c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [56 24  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [47 62 71  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]] [2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 3, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(index_inputs[:3], inputs_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650789af-7ba3-47e4-b04a-f178873dba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, '', sequence)\n",
    "        sequence_index = [dictionary[SOS]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) > MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        \n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f30233-0cdf-4db5-81a3-4014ea8fd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6792f22c-8bc7-459b-aaf2-c0b220fdd34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  85 105  23  77   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  85 105  23  77   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1  90  74   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]] [5, 5, 3, 5, 3, 5, 6, 6, 5, 3, 5, 4, 5, 7, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(index_outputs[:3], output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3bf667b-9a5b-45e1-a133-8500022d38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
    "    sequences_target_index = []\n",
    "\n",
    "    if tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, '', sequence)\n",
    "        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) >= MAX_SEQUENCE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUENCE-1] + [dictionary[END]] # 기존보다 1만큼 길이를 줄이고 END 토큰 추가\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]] # 넘지 않을 때, padding 붙여준 다음 END를 바로 붙여준다.(PAD 토큰 추가할 필요없이 문장 마지막에 END 토큰 추가)\n",
    "\n",
    "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]] # PAD 토큰 추가\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5685ffa-98c8-4728-9332-1f0a236c6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90176b5a-2c44-4981-8448-5189ce7c36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['char2idx'] = char2idx\n",
    "data_configs['idx2char'] = idx2char\n",
    "data_configs['vocab_size'] = vocab_size\n",
    "data_configs['pad_symbol'] = PAD\n",
    "data_configs['sos_symbol'] = SOS\n",
    "data_configs['end_symbol'] = END\n",
    "data_configs['unk_symbol'] = UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3776d7a9-9c0b-467a-b480-e506b44fc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = '/Users/jeon-yewon/Desktop/데이터 분석 강의/부트캠프/12주차/data_in/'\n",
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1798fe20-e9bd-429f-92fc-d21427f933a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'wb'), index_outputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_TARGETS, 'wb'), index_targets)\n",
    "\n",
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474c391-d817-4788-a43d-768202d0844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a49668-e860-4ba7-81d2-4b3b44f2185f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20e951-a637-4ca9-a2ba-646667c7a06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
