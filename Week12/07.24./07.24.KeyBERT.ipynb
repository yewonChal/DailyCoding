{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3a6402-f304-424d-8a63-b3199e7e0373",
   "metadata": {},
   "source": [
    "# KeyBERT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6108e9b1-bfb3-4a1a-8baf-623c8f5f257b",
   "metadata": {},
   "source": [
    ": 기존 TF-IDF, LSA, LDA와 같은 방식보다 더 높은 정확도를 제공한다.\n",
    "  (BERT를 사용하여 핵심 키워드 추출)\n",
    "\n",
    "- 문서 임베딩 생성 => 키워드 후보 생성 => 유사도 계산 => 키워드 선택"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48e0244d-b34e-4386-83db-7c5877fc6a13",
   "metadata": {},
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e0fa421-20ad-4c53-b11e-4d76a75d039e",
   "metadata": {},
   "source": [
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac12a7db-ada3-4142-8fb0-eb24fa97450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeon-yewon/miniforge3/envs/tensorflow/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cdd35ca43f48e8a61af84df7f9f731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89cff69fa9b43119059fc79a4dda15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf0dc9b08f549b581a14d82a81d7696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcc027eca184edeb8bfed7377c5f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce59c63a5344ba18aac15d6dd9d3541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bcb7d159164a949c8ff6c5f00fbfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1049a2e96d34655a7fa26ef4484b310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e6dfd1947542e8a6c24dc4a0967036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50db6c91b124ee3815a7a455ab9f6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f4ccd6571f4ace94bc373122947584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c770460ec994ceaab5391e3b9eee828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b02b52592646c1b0f1d60a02f8516c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c2b8b3-07ee-4374-82b3-3f2b2a0dc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('easy', 0.5055), ('keyphrases', 0.476), ('minimal', 0.4497), ('keyword', 0.3599), ('technique', 0.3567)]\n"
     ]
    }
   ],
   "source": [
    "doc = 'KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.'\n",
    "\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f96dce-2895-4cd8-88e1-d0d8b7aaff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformers 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a62845-78d6-4f06-a015-5c23713b91a6",
   "metadata": {},
   "source": [
    "# SBERT(Sentence-BERT)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b78e1a0-ba2d-4657-8d03-b192be8062ee",
   "metadata": {},
   "source": [
    ": BERT를 활용한 문장 임베딩 처리(유사도 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397a327d-28fb-475e-bf91-50672e7ed2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f1e39468ec48cca954e1f707c9b31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddd143d9aab4538815f4fe2c954b39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce0fa198c664605ba693f958e06434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64929782f7f34a2a92d650fdbc11a2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6bbb8948804cd4a19fb57f293483fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62fe20fa8014783b73a79e9787134b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b3c9cb8d1740e5845ed26654527a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12741317fa9c4d3392c8bb66ca25fa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5e79090b4b43279aa351a5ed590010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f82298f708845759f4e4f5006b8d793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869f1b40a2514195bfc6d181026e5ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d9f082-0038-41a4-8e48-3436fbce933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'This is an example sentence'\n",
    "sentence2 = 'This is a sample sentence'\n",
    "\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93740688-fdbd-4790-9ce8-cede959ede80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코사인 유사도 :  0.7881680130958557\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "\n",
    "print('코사인 유사도 : ', cosine_similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ddfd85-2c92-49a5-9244-aa4d95bd9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bd9a17-5be8-4a9b-b2a3-c25baf90176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca25facf-a864-4274-a318-cd65e79a5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 :  72\n",
      "['algorithm analyzes training' 'algorithm correctly determine'\n",
      " 'algorithm generalize training' 'allow algorithm correctly'\n",
      " 'analyzes training data']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (3, 3) # trigram 사용\n",
    "stop_words = 'english'\n",
    "\n",
    "# CountVectorizer을 사용하던 n-gram을 쉽게 추출할 수 있다.\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 : ', len(candidates))\n",
    "print(candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7492f08a-75fa-4850-bfbf-4b9fbe272232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9620a3d-9352-4851-9989-c6df8de33f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 통해 해당 trigram들과 유사도가 높으면\n",
    "# 문서를 대표할 수 있다는 의미를 나타낸다.\n",
    "top_n = 5\n",
    "\n",
    "dictances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in dictances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716bedf8-78d4-49e3-bad8-03da62a648c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training examples supervised', 'function labeled training', 'supervised learning machine', 'supervised learning example', 'supervised learning algorithm']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4514fc9-8202-4856-9e67-2a63ad6a1ff2",
   "metadata": {},
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 후보 키워드들 간의 유사도\n",
    "    distances = cosine_similarity([doc_embedding], candidate_embeddings)[0]\n",
    "\n",
    "    # 각 후보 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 상위 nr_candidates 개의 키워드들을 뽑는다.\n",
    "    words_idx = list(distances.argsort()[-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 연관성이 낮은 키워드 찾기\n",
    "    min_sim = np.inf  # 최소값 설정(np.inf : 무한대)\n",
    "    candidate = None  # 키워드 저장 변수\n",
    "\n",
    "    # itertools.combinations : 상위 top_n개의 조합\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])  # 각 조합에 대한 유사도 합산(자기 자신 제외)\n",
    "        if sim < min_sim:  # 최솟값도 낮은 유사도\n",
    "            candidate = combination  # 최적 후보로 등록\n",
    "            min_sim = sim  # 최솟값 변경\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]  # 리스트 내 데이터 뽑아서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e11eb952-1b78-4321-8f3c-11f037c2ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # doc_embedding을 2차원 배열로 변환\n",
    "    doc_embedding = np.array(doc_embedding).reshape(1, -1)\n",
    "    \n",
    "    # 문서와 각 후보 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)[0]\n",
    "\n",
    "    # 각 후보 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 상위 nr_candidates 개의 키워드들을 뽑는다.\n",
    "    words_idx = list(distances.argsort()[-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 연관성이 낮은 키워드 찾기\n",
    "    min_sim = np.inf  # 최소값 설정(np.inf : 무한대)\n",
    "    candidate = None  # 키워드 저장 변수\n",
    "\n",
    "    # itertools.combinations : 상위 top_n개의 조합\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])  # 각 조합에 대한 유사도 합산(자기 자신 제외)\n",
    "        if sim < min_sim:  # 최솟값도 낮은 유사도\n",
    "            candidate = combination  # 최적 후보로 등록\n",
    "            min_sim = sim  # 최솟값 변경\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]  # 리스트 내 데이터 뽑아서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a496542b-3c05-48fd-bcbb-2667271a2c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task learning function',\n",
       " 'supervisory signal supervised',\n",
       " 'labeled training data',\n",
       " 'examples supervised learning',\n",
       " 'supervised learning algorithm']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 10개의 키워드를 뽑고 이 중 연관성이 낮은 5개를 선택하겠다.\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f825ab3-a752-4455-b720-0fa5dc0889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance : 텍스트 요약 처리(중복 최소화)\n",
    "def max_mar_rel(doc_embedding, cadidate_embeddings, words, top_n, diversity):\n",
    "    # 문서의 각 키워드들 간의 유사도\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 가장 높은 유사도를 가진 문서의 인덱스\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    # 가장 높은 유사도를 제외한 인덱스들\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # top_n - 1 : 이미 가장 높은 유사도는 추출\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :] # 각 키워드들 간의 유사도\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1) # 가장 높은 유사도와 유사도 계산\n",
    "\n",
    "        # MMR 점수 계산 (1 - diversity : 문서와의 관련성)\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)] # 최고 MMR\n",
    "\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7b79ea9-e4f4-4e12-a59c-9c607d641d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['supervised learning algorithm',\n",
       " 'function labeled training',\n",
       " 'supervised learning example',\n",
       " 'supervisory signal supervised',\n",
       " 'supervised learning machine']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mar_rel(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ad1b387-472b-4656-8713-23d0d2aec147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "doc = \"\"\"\n",
    "         드론 활용 범위도 점차 확대되고 있다. 최근에는 미세먼지 관리에 드론이 활용되고 있다.\n",
    "         서울시는 '미세먼지 계절관리제' 기간인 지난달부터 오는 3월까지 4개월간 드론에 측정장치를 달아 미세먼지 집중 관리를 실시하고 있다.\n",
    "         드론은 산업단지와 사업장 밀집지역을 날아다니며 미세먼지 배출 수치를 점검하고, 현장 모습을 영상으로 담는다.\n",
    "         영상을 통해 미세먼지 방지 시설을 제대로 가동하지 않는 업체와 무허가 시설에 대한 단속이 한층 수월해질 전망이다.\n",
    "         드론 활용에 가장 적극적인 소방청은 광범위하고 복합적인 재난 대응 차원에서 드론과 관련 전문인력 보강을 꾸준히 이어가고 있다.\n",
    "         지난해 말 기준 소방청이 보유한 드론은 총 304대, 드론 조종 자격증을 갖춘 소방대원의 경우 1,860명이다.\n",
    "         이 중 실기평가지도 자격증까지 갖춘 ‘드론 전문가’ 21명도 배치돼 있다.\n",
    "         소방청 관계자는 \"소방드론은 재난현장에서 영상정보를 수집, 산악ㆍ수난 사고 시 인명수색·구조활동,\n",
    "         유독가스·폭발사고 시 대원안전 확보 등에 활용된다\"며\n",
    "         \"향후 화재진압, 인명구조 등에도 드론을 활용하기 위해 연구개발(R&D)을 하고 있다\"고 말했다.\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e561f818-5b40-4850-a1ce-292303507e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n         ', 'Foreign'), ('드론', 'Noun'), ('활용', 'Noun'), ('범위', 'Noun'), ('도', 'Josa'), ('점차', 'Noun'), ('확대', 'Noun'), ('되고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n",
      "드론 활용 범위 점차 확대 최근 미세먼지 관리 드론 활용 서울시 미세먼지 계절 관리제 기간 지난달 개 월간 드론 측정 장치 달 미세먼지 집중 관리 실시 드론 산업 단지 사업 밀집 지역 미세먼지 배출 수치 점검 현장 모습 영상 영상 통해 미세먼지 방지 시설 제대로 가동 업체 무허가 시설 대한 단속 한층 전망 드론 활용 가장 적극 소방청 복합 재난 대응 차원 드론 관련 전문 인력 보강 어가 지난해 말 기준 소방청 보유 드론 총 드론 조종 자격증 소방대 경우 명 이 중 실기 평가 지도 자격증 드론 전문가 명도 배치 소방청 관계자 소방 드론 재난 현장 영상 정보 수집 산악 수난 사고 시 인명 수색 구조 활동 유독가스 폭발사고 시 대원 안전 확보 등 활용 며 향후 화재 진압 인명구조 등 드론 활용 위해 연구개발 고 말\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "tokenized_doc = okt.pos(doc)\n",
    "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "\n",
    "print(tokenized_doc[:10])\n",
    "print(tokenized_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0481f41-c49f-4fd0-8c5b-6769cdb0d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram 개수 : 222\n",
      "['가동 업체' '가동 업체 무허가' '가장 적극' '가장 적극 소방청' '경우 실기']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3) # bigram, trigram\n",
    "\n",
    "count = CountVectorizer(ngram_range = n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('n-gram 개수 :', len(candidates))\n",
    "print(candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2e9738e-20e3-4556-8339-315e0bedbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15d176e4-91ea-44be-b258-5d8b0a66c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bda4b7aa-0144-40ec-956c-cf344a643146",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29980546-743b-4b11-a6dc-637d6b67de32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['드론 산업', '드론 드론 조종', '실시 드론 산업', '관리 드론 활용', '미세먼지 관리 드론']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c899bdf-5322-4ec0-9919-5332ee3eedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['드론 산업 단지', '전망 드론 활용', '드론 산업', '관리 드론 활용', '미세먼지 관리 드론']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd8cfb1e-d065-4bb6-ac7d-77e7446c2427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미세먼지 관리 드론', '실시 드론 산업', '관리 드론 활용', '월간 드론 측정', '전망 드론 활용']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mar_rel(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efb143-30e6-4f16-99d3-5f42cdf882a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
