{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f8dca5-10b2-40ea-8794-b91861d477bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b859285-d2c3-44a7-b595-7f690ef62ec4",
   "metadata": {},
   "source": [
    "# 토큰화(tokenization) : 구두점을 제거하고 토큰의 기준(보통 단어) 문자열을 나눠주는 것\n",
    " - 구두점 제거에서 단어 사이의 공백, 특수기호(마침표), 줄임말 등을 단순히 제거하는 것이 아니라고 하나의 토큰으로 혹은 다른 토큰에 포함될 수 있도록 처리가 가능하여야 한다.\n",
    "\n",
    "# 정체(cleaning) : 노이즈 데이터를 제거한다.\n",
    " - 불필요한 단어를 제거(등장 빈도가 적다. 길이가 짧다.)\n",
    "\n",
    "# 정규화(normalization) : 같은 의미를 가진 다른 단어들을 같은 단어로 통합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cccb0c2-ef3a-43f4-b45b-83635a5bcde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jeon-\n",
      "[nltk_data]     yewon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출, 어간 추출\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4dc6e2-7c18-4249-97c6-cbb4ccfdf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 :  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 :  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print('표제어 추출 전 : ', words)\n",
    "print('표제어 추출 후 : ', [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d12934d-7258-4136-abe3-3fcfe10b840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dde071-a096-40d8-8557-bccbf2668bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'live'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('lives', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ec6569-ea54-45df-a000-8c3682671e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1374a0ef-9752-46df-a3f2-b8aed6b1ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd0f16b-ceaf-439d-b2a8-c692e9df327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 :  ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 :  ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 : ', tokenized_sentence)\n",
    "print('어간 추출 후 : ', [stemmer.stem(word) for word in tokenized_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1faf730-1b67-4431-bdb4-1629a774abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 :  179\n",
      "불용어 예시 :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print('불용어 개수 : ', len(stop_words))\n",
    "print('불용어 예시 : ', stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3fcb5cd-5f6e-4be5-8715-d3519de7b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 :  ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        result.append(word)\n",
    "\n",
    "print('불용어 제거 전 : ', word_tokens)\n",
    "print('불용어 제거 후 : ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f37c663-22e0-40f5-b107-650ec6c89cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# . 임의의 문자 한 개\n",
    "r = re.compile('a.c')\n",
    "r.search('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b564cdc-22b0-4d5e-87a4-1daf568e80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 3), match='a-c'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('abc'))\n",
    "print(r.search('a-c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13ff184-3b61-46ab-8cd8-48742e1430d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? : 존재할 수도 있고, 존재하지 않을 수도 있을 경우\n",
    "r = re.compile('ab?c')\n",
    "r.search('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d27a92-8e74-4845-a635-c8998e4ef977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ac'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('ac'))\n",
    "print(r.search('abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b563ee66-6f0a-4d95-a846-afcf8097558b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(1, 3), match='ac'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * : (앞의 문자가) 0개 이상인 경우.\n",
    "r = re.compile('ab*c')\n",
    "r.search('aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833b9dae-711e-41ca-a79f-53582f2b6048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ac'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 12), match='abbbbbbbbbbc'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('ac'))\n",
    "print(r.search('abc'))\n",
    "print(r.search('abbbbbbbbbbc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a870226f-d9c9-4e06-bec2-8b4a098b9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# + : (앞의 문자가) 1개 이상인 경우.\n",
    "r = re.compile('ab+c')\n",
    "r.search('aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5e4fc30-daa7-40ae-b9b6-a561d350808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 11), match='abbbbbbbbbc'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('ac'))\n",
    "print(r.search('abc'))\n",
    "print(r.search('abbbbbbbbbc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea37090c-d790-4b45-b8e8-ad31d706d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ : 문자열의 시작\n",
    "r = re.compile('^ab')\n",
    "\n",
    "r.search('aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0c63ee9-0c28-462e-b9e4-645126321be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('cab'))\n",
    "print(r.search('aac'))\n",
    "print(r.search('ababab'))\n",
    "print(r.search('abbbbbbbbb'))\n",
    "print(r.search('abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10f81640-155b-4964-952f-6c7b9c09ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (숫자) : (앞의 문자가) '숫자 개'인 경우\n",
    "r = re.compile('ab{2}c')\n",
    "\n",
    "r.search('aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d559924-9f65-4e84-bf6d-2fa60fdc243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(r.search('ac'))\n",
    "print(r.search('abc'))\n",
    "print(r.search('abbc'))\n",
    "print(r.search('abbbbbbbbbc'))\n",
    "print(r.search('abccc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae61a67-3ccd-4747-bee7-a6bec6986f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {숫자1, 숫자2} : (앞의 문자가) 숫자1 이상 숫자2 이하의 개수인 경우\n",
    "r = re.compile('ab{2, 5}c')\n",
    "\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e535cd52-69be-4874-a87f-08466f6ba8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(r.search('abbc'))\n",
    "print(r.search('abbbbbc'))\n",
    "print(r.search('abbbbbbbbbbc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c02b06-a709-49f3-935b-a7f49e1ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {숫자, } : (앞의 문자가) 해당 숫자 이상의 개수인 경우\n",
    "# {0, } == *, {1, } == +\n",
    "r = re.compile('a{2,}bc')\n",
    "\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2110a3e4-ac9c-4086-818f-780f1830a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='aabc'>\n",
      "<re.Match object; span=(0, 13), match='aaaaaaaaaaabc'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('aabc'))\n",
    "print(r.search('aaaaaaaaaaabc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb1e6888-28c8-4c56-b7f8-ab040e3bade3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [] : 범위 내에 포함된 문자가 있는 경우\n",
    "r = re.compile('[abc]')\n",
    "r.search('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c620d339-d20f-4f8a-8681-680539141943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='c'>\n",
      "<re.Match object; span=(1, 2), match='a'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('aaaaaaaaaaaaaaa'))\n",
    "print(r.search('z'))\n",
    "print(r.search('abc'))\n",
    "print(r.search('cbac'))\n",
    "print(r.search('zaca'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a05cc5c-f859-40d0-a3b6-b71ab917f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [a-z][A-Z][0-9]\n",
    "r = re.compile('[a-z]')\n",
    "\n",
    "r.search('AAA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e134772-25ae-491c-8968-6631ecd54565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(2, 3), match='c'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('1234'))\n",
    "print(r.search('A12B12'))\n",
    "print(r.search('ABc'))\n",
    "print(r.search('a12b12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf2a5b7e-88c8-4fe1-8b30-cd762c539024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [^문자] : ^ 뒤에 있는 문자들을 제외시킨다.\n",
    "r = re.compile('[^abc]')\n",
    "\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9a81e4c-6e35-4016-827a-ac9e75fc2309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 2), match='1'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='z'>\n",
      "<re.Match object; span=(0, 1), match='1'>\n"
     ]
    }
   ],
   "source": [
    "print(r.search('a12b12'))\n",
    "print(r.search('A12B12'))\n",
    "print(r.search('z'))\n",
    "print(r.search('1234'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2805a958-dbcf-48ef-8107-ef576e19be9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(4, 7), match='abc'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.match() : 시작, re.search() : 전체\n",
    "r = re.compile('ab.')\n",
    "\n",
    "r.search('aaaaabc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "422867c6-b57b-470c-95f8-27c5ce98bf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(2, 5), match='abb'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(r.search('abc'))\n",
    "print(r.search('aaabbcc'))\n",
    "print(r.search('cba'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "371a59ca-4805-4554-acfa-a585fd98ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정규', '표현식', '또는', '정규식은', '특정한', '규칙을', '가진', '문자열의', '집합']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '정규 표현식 또는 정규식은 특정한 규칙을 가진 문자열의 집합'\n",
    "\n",
    "re.split(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63f9b10c-c0ec-42bc-bf3c-4e89724f9ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정규', '표현식', '또는', '정규식은', '특정한', '규칙을', '가진', '문자열의', '집합']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''정규\n",
    "표현식\n",
    "또는\n",
    "정규식은\n",
    "특정한\n",
    "규칙을\n",
    "가진\n",
    "문자열의\n",
    "집합'''\n",
    "\n",
    "re.split('\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c208d430-8bab-4494-9685-5f3a3c4bf4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정규', '표현식', '또는', '정규식은', '특정한', '규칙을', '가진', '문자열의', '집합']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '정규+표현식+또는+정규식은+특정한+규칙을+가진+문자열의+집합'\n",
    "\n",
    "re.split('\\+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1498e4b6-c3fd-4113-a727-5499ab78eef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20', '010', '1234', '5678']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''이름 : 홍길동\n",
    "나이 : 20\n",
    "전화번호 : 010-1234-5678\n",
    "학교 : 길동대학교\n",
    "'''\n",
    "\n",
    "re.findall('\\d+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f58817b-fe69-4f3f-9624-2b4cd2f66ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+', '정규 표현식 또는 정규식')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84ff8c27-5256-40dd-b781-fd02a46ca29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  :   ,   [1] (    )[2][3] ,        ,         .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.sub() : 정규 표현식과 일치하는 문자열을 다른 문자열로 대체\n",
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "# 알파벳 제거\n",
    "re.sub('[a-zA-Z]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38aa7b50-9b36-40e6-8315-9b7ffa3b7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"\n",
    "\n",
    "# 텍스트 전처리 예제\n",
    "re.split('\\s+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa6b93cc-1592-4550-8e60-7ec828613b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "122d89f0-abae-4f03-99c2-7968fae6d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09906938-4d3a-486b-a1bb-0c5e2ca61eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}', text) # 대문자가 연속으로 4개 발생한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e71cea6-b828-4870-8455-5274e143b06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d9826-8347-4c22-baff-d33cf6009ff1",
   "metadata": {},
   "source": [
    "### 문자 표현\n",
    "### 문자 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dafa760b-48bd-4aa2-916c-f45e914627b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "853a24c5-301a-4652-a7e1-b59e57b2f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'barber': 1, 'is': 2, 'a': 3, 'person.': 4, 'good': 5, 'huge': 6, 'he': 7, 'Knew': 8, 'Secret!': 9, 'The': 10, 'Secret': 11, 'He': 12, 'Kept': 13, 'secret.': 14, 'Huge': 15, 'His': 16, 'kept': 17, 'his': 18, 'word.': 19, 'But': 20, 'keeping': 21, 'and': 22, 'such': 23, 'secret': 24, 'to': 25, 'himself': 26, 'was': 27, 'driving': 28, 'the': 29, 'crazy.': 30, 'went': 31, 'up': 32, 'mountain.': 33}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "index = 0\n",
    "\n",
    "for text in sentences:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc92ed5a-92a3-4dc6-a367-c8f40f005e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "\n",
    "    preprocessed_sentences.append(result)\n",
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07be1b85-4f01-4fd6-9ccd-7cce2f627dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0342245-eacf-4b44-a302-3eb1d6240bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['barber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1187489a-05af-4b57-be75-1c0d8828f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(sorted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b8901a1-fb57-4da3-bdce-7bfe56b1303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in sorted_vocab:\n",
    "    if frequency > 1:\n",
    "        i = i+1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "256558da-78b4-4877-8d66-9bd12e410f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# person까지만 딕셔너리에 넣어주고 싶다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61983725-ad98-4248-bb60-ee247fb038a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency = [word for word, index in word_to_index.items() if index >= 5 + 1]\n",
    "\n",
    "for w in word_frequency:\n",
    "    del word_to_index[w]\n",
    "\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13241c3a-f5df-488d-b08a-fee2726b91c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'NaN': 6}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['NaN'] = len(word_to_index) + 1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71614258-2a38-42f3-be4b-769871a026ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            encoded_sentence.append(word_to_index['NaN'])\n",
    "\n",
    "    encoded.append(encoded_sentence)\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ac727fe-99c8-450f-bb53-7c13a9a5804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_words = sum(preprocessed_sentences, [])\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2e229e9-f7b3-43d9-ac57-8d486a59db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(all_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ac029f6-f777-4ff1-9aae-00fabc897ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab.most_common(5)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "903bb62c-6b57-416c-ba33-d72971106f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1b14ed8-8506-4341-a6cf-cc90094e69dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber' 'person' 'barber' 'good' 'person' 'barber' 'huge' 'person'\n",
      " 'knew' 'secret' 'secret' 'kept' 'huge' 'secret' 'huge' 'secret' 'barber'\n",
      " 'kept' 'word' 'barber' 'kept' 'word' 'barber' 'kept' 'secret' 'keeping'\n",
      " 'keeping' 'huge' 'secret' 'driving' 'barber' 'crazy' 'barber' 'went'\n",
      " 'huge' 'mountain']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "print(np.hstack(preprocessed_sentences))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80c6a75c-9b0b-4423-a4bb-1ba525bc7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab.most_common(5)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56d0c9eb-b9e2-494c-ae91-7b83c926e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "357d494d-a4c2-4340-8017-71751330ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56792a69-26ab-4f77-8501-53ee8a142dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e80ccf5e-5d79-4999-85ac-b810d1ac3580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdbf700f-f0d5-4bc1-960c-4481c65069f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ff3bc00-098f-4b36-8e1f-cf6e151f101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 6) # 상위 5개 단어\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41ac462e-1cce-4e9f-a49e-3ef250578932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "373a96e7-2a24-4026-bc10-6d77fa04cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "440f05e6-e3fc-4844-b151-947d81169490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 6, oov_token = 'OOV') # Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e5febd0-eb7e-4dc7-8074-1f9cc8c891a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "793b5f4a-b28a-4278-8707-a6c8bdd89b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da244191-76cc-41d9-bb15-40494a277c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1], [2, 1, 1], [2, 4, 1], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "encoded = print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9409e846-fb0d-40cb-a7b8-1716759527b6",
   "metadata": {},
   "source": [
    "max_len = max(len(item) for item in encoded)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d4a97d5-64e2-46ff-a802-540aaff181a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Example list to represent the 'encoded' variable\n",
    "encoded = ['this', 'is', 'an', 'example', 'list']\n",
    "\n",
    "# Ensure 'encoded' is not None and is iterable\n",
    "if encoded is not None:\n",
    "    max_len = max(len(item) for item in encoded)\n",
    "    print(max_len)\n",
    "else:\n",
    "    print(\"The 'encoded' variable is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aba43e6a-c521-41aa-8c54-a0dd19cf0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def encode_sentences(sentences):\n",
    "    # Example encoding process (could be tokenization, stemming, etc.)\n",
    "    return [sentence for sentence in sentences if sentence]  # Filter out empty sentences\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is a sentence.\", \"Another one.\", \"And yet another sentence.\"]\n",
    "\n",
    "# Encode sentences\n",
    "encoded = encode_sentences(sentences)\n",
    "\n",
    "# Check if 'encoded' is not None before processing\n",
    "if encoded is not None:\n",
    "    max_len = max(len(item) for item in encoded)\n",
    "    print(max_len)\n",
    "else:\n",
    "    print(\"The 'encoded' variable is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3118120-d8d9-4fa0-8573-b15fbc947b25",
   "metadata": {},
   "source": [
    "## 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9674e988-1d41-4dce-b5dd-ffaee75be5c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10961,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;241m<\u001b[39m max_len:\n\u001b[1;32m      6\u001b[0m         sentence\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m padding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(padding)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10961,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for sentence in encoded:\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(0)\n",
    "\n",
    "padding = np.array(encoded)\n",
    "print(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eebfd493-ef9d-4858-a1cb-fba8dc9b2ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10961,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m padding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m padding\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10961,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "padding = np.array(encoded)\n",
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "349c1ad1-29c4-4f60-8591-942169a22b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10880, 147],\n",
       " [10880, 10, 147],\n",
       " [10880, 1343, 147],\n",
       " [629, 753],\n",
       " [753, 764, 1343, 753],\n",
       " [1343, 753],\n",
       " [10880, 764, 237],\n",
       " [10880, 764, 237],\n",
       " [10880, 764, 753],\n",
       " [1204, 1204, 1343, 753, 971, 10880, 4053],\n",
       " [10880, 162, 1343, 3320]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c959b008-ddf5-418a-bb1b-d54364f4e17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0, 10880,   147],\n",
       "       [    0,     0,     0,     0, 10880,    10,   147],\n",
       "       [    0,     0,     0,     0, 10880,  1343,   147],\n",
       "       [    0,     0,     0,     0,     0,   629,   753],\n",
       "       [    0,     0,     0,   753,   764,  1343,   753],\n",
       "       [    0,     0,     0,     0,     0,  1343,   753],\n",
       "       [    0,     0,     0,     0, 10880,   764,   237],\n",
       "       [    0,     0,     0,     0, 10880,   764,   237],\n",
       "       [    0,     0,     0,     0, 10880,   764,   753],\n",
       "       [ 1204,  1204,  1343,   753,   971, 10880,  4053],\n",
       "       [    0,     0,     0, 10880,   162,  1343,  3320]], dtype=int32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pad_sequences(encoded)\n",
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0bc1ceb-6208-470e-af70-ffdad4002c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10880,   147,     0,     0,     0],\n",
       "       [10880,    10,   147,     0,     0],\n",
       "       [10880,  1343,   147,     0,     0],\n",
       "       [  629,   753,     0,     0,     0],\n",
       "       [  753,   764,  1343,   753,     0],\n",
       "       [ 1343,   753,     0,     0,     0],\n",
       "       [10880,   764,   237,     0,     0],\n",
       "       [10880,   764,   237,     0,     0],\n",
       "       [10880,   764,   753,     0,     0],\n",
       "       [ 1343,   753,   971, 10880,  4053],\n",
       "       [10880,   162,  1343,  3320,     0]], dtype=int32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pad_sequences(encoded, padding='post', maxlen=5) # post : 앞에서부터 채우기, maxlen : 전체 길이\n",
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "85a1ea8d-ea3a-4432-9c11-9fcd02349729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10880,   147,     0,     0,     0],\n",
       "       [10880,    10,   147,     0,     0],\n",
       "       [10880,  1343,   147,     0,     0],\n",
       "       [  629,   753,     0,     0,     0],\n",
       "       [  753,   764,  1343,   753,     0],\n",
       "       [ 1343,   753,     0,     0,     0],\n",
       "       [10880,   764,   237,     0,     0],\n",
       "       [10880,   764,   237,     0,     0],\n",
       "       [10880,   764,   753,     0,     0],\n",
       "       [ 1204,  1204,  1343,   753,   971],\n",
       "       [10880,   162,  1343,  3320,     0]], dtype=int32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pad_sequences(encoded, padding='post', truncating='post', maxlen=5) # truncating : 뒤에서부터 데이터 삭제\n",
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "18610e80-1793-43ad-997c-55f46828bdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'and': 2, 'in': 3, 'this': 4, 'was': 5, 'not': 6, 'map': 7, 'we': 8, 'found': 9, 'billy': 10, \"bones's\": 11, 'chest': 12, 'but': 13, 'an': 14, 'accurate': 15, 'copy': 16, 'complete': 17, 'all': 18, 'things': 19, 'names': 20, 'heights': 21, 'soundings': 22, 'with': 23, 'single': 24, 'exception': 25, 'of': 26, 'red': 27, 'crosses': 28, 'written': 29, 'notes': 30}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16fba5d4-b640-4fb5-91c9-ff71e234001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 9, 16, 3, 18, 29]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_text = 'the map found copy in all written'\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d84859c-40b8-4c58-9a41-2e494aaccc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "440ab343-4c41-4d55-afc9-0c55ecb63864",
   "metadata": {},
   "source": [
    "# Bag of Words(BoW), TF-IDF, DTM(단어의 빈도수를 카운트)(Document-Term Matrix) : 극소표현 Local Representation\n",
    "# 워드 임베딩(예측을 기반으로 작업) : 연속 표현 Continuous Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "493b9201-cdb2-4cb2-b34c-8749a000c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'yout': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "# Bad of Words : 단어들의 순서를 전혀 고려하지 않고 출현 빈도에만 초점을 맞춘 수치화 방법\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want yout love. because i love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print('Bag of Words : ', vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1473756c-0c28-4fa8-91b4-070290fe1a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The cat sat on the mat', 'The dog ate my notebook', 'The cat chased the dog']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "X = vector.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6531ad28-d94d-4d01-a5c2-19eaa8f2a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate' 'cat' 'chased' 'dog' 'mat' 'my' 'notebook' 'on' 'sat' 'the']\n",
      "[[0 1 0 0 1 0 0 1 1 2]\n",
      " [1 0 0 1 0 1 1 0 0 1]\n",
      " [0 1 1 1 0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a596d8c5-7af8-4c4c-b697-6c01a67e21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=['the', 'a', 'an', 'is', 'not'])\n",
    "print('Bag of Words : ', vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "45b011b2-9d15-41a5-a116-97d459086b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "print('Bag of Words : ', vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "92279309-a206-40b9-9170-927cd2104ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "vect = CountVectorizer(stop_words = stop_words)\n",
    "print('Bag of Words : ', vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "452c3b61-0836-40ff-8572-353a5b6b0125",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "tf(d,t) = 문서 d에서의 단어 t의 등장 횟수\n",
    "df(t) = 단어 t가 등장한 문서 수\n",
    "idf(t) : df(t)에 반비례하는 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6cb94d07-c81d-4c1e-b1cb-7fd2dd64d7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기차는', '길다', '맛있다', '바나나는', '빨갛다', '사과는']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "    '사과는 빨갛다',\n",
    "    '사과는 맛있다',\n",
    "    '바나나는 길다',\n",
    "    '바나나는 맛있다',\n",
    "    '기차는 길다'\n",
    "]\n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2cd4d304-6e96-4074-a988-1252f42f4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4c008dd1-b020-4d58-ac6f-caaf02d0fc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기차는</th>\n",
       "      <th>길다</th>\n",
       "      <th>맛있다</th>\n",
       "      <th>바나나는</th>\n",
       "      <th>빨갛다</th>\n",
       "      <th>사과는</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   기차는  길다  맛있다  바나나는  빨갛다  사과는\n",
       "0    0   0    0     0    1    1\n",
       "1    0   0    1     0    0    1\n",
       "2    0   1    0     1    0    0\n",
       "3    0   0    1     1    0    0\n",
       "4    1   1    0     0    0    0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1732365b-1152-4685-8e9f-ee0694aefb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>기차는</th>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길다</th>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>맛있다</th>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나는</th>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>빨갛다</th>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과는</th>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           IDF\n",
       "기차는   0.916291\n",
       "길다    0.510826\n",
       "맛있다   0.510826\n",
       "바나나는  0.510826\n",
       "빨갛다   0.916291\n",
       "사과는   0.510826"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=['IDF'])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d7468b6e-8a40-472b-9867-cc41603ee384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기차는</th>\n",
       "      <th>길다</th>\n",
       "      <th>맛있다</th>\n",
       "      <th>바나나는</th>\n",
       "      <th>빨갛다</th>\n",
       "      <th>사과는</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.916291</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        기차는        길다       맛있다      바나나는       빨갛다       사과는\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.916291  0.510826\n",
       "1  0.000000  0.000000  0.510826  0.000000  0.000000  0.510826\n",
       "2  0.000000  0.510826  0.000000  0.510826  0.000000  0.000000\n",
       "3  0.000000  0.000000  0.510826  0.510826  0.000000  0.000000\n",
       "4  0.916291  0.510826  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t, d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bf0012cc-d503-4e93-af63-224ce0267bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 0 0 1 1 2]\n",
      " [1 0 0 1 0 1 1 0 0 1]\n",
      " [0 1 1 1 0 0 0 0 0 2]]\n",
      "{'the': 9, 'cat': 1, 'sat': 8, 'on': 7, 'mat': 4, 'dog': 3, 'ate': 0, 'my': 5, 'notebook': 6, 'chased': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "courpus = [\n",
    "    'you know i want your love',\n",
    "    'I like you',\n",
    "    'what should i do'\n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7a29d735-6a76-4064-a4de-e4f182644637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.34101521 0.         0.         0.44839402 0.\n",
      "  0.         0.44839402 0.44839402 0.52965746]\n",
      " [0.50461134 0.         0.         0.38376993 0.         0.50461134\n",
      "  0.50461134 0.         0.         0.29803159]\n",
      " [0.         0.40352536 0.53058735 0.40352536 0.         0.\n",
      "  0.         0.         0.         0.62674687]]\n",
      "{'the': 9, 'cat': 1, 'sat': 8, 'on': 7, 'mat': 4, 'dog': 3, 'ate': 0, 'my': 5, 'notebook': 6, 'chased': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "print(tfidfv.fit_transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dc4a693-8881-413c-912c-90a02c9fa655",
   "metadata": {},
   "source": [
    "# 희소표현 : 바나나 [0 0 0 1 0 0 0...]\n",
    "# 밀집표현 : 바나나[0.2 -1 1.8 2.9 -3.1...]\n",
    "\n",
    "단어를 밀집 표현 벡터의 형태로 변환하는 것을 워드 임베딩이라고 한다.\n",
    "해당 과정에서 나온 결과 벡터를 임베딩 벡터라고 한다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4cee806-f906-4057-b1f5-d8c11e73d7c3",
   "metadata": {},
   "source": [
    "CBOW(Continuous Bag of Words) : 주변에 있는 단어들을 기준으로 중간에 있는 단어 예측\n",
    "Skip-Gram : 중간에 있는 단어들을 기준으로 주변 단어 예측\n",
    "\n",
    "you knoe i want your love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "739e656e-8268-468c-8856-1bead76d5e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'dog', 'ate', 'my', 'notebook'],\n",
       " ['the', 'cat', 'chased', 'the', 'dog']]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['The cat sat on the mat', 'The dog ate my notebook', 'The cat chased the dog']\n",
    "\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5cd367a8-b500-42a4-9f5b-c46919fd8102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01723938,  0.00733148,  0.01037977,  0.01148388,  0.01493384,\n",
       "       -0.01233535,  0.00221123,  0.01209456, -0.0056801 , -0.01234705,\n",
       "       -0.00082045, -0.0167379 , -0.01120002,  0.01420908,  0.00670508,\n",
       "        0.01445134,  0.01360049,  0.01506148, -0.00757831, -0.00112361,\n",
       "        0.00469675, -0.00903806,  0.01677746, -0.01971633,  0.01352928,\n",
       "        0.00582883, -0.00986566,  0.00879638, -0.00347915,  0.01342277,\n",
       "        0.0199297 , -0.00872489, -0.00119868, -0.01139127,  0.00770164,\n",
       "        0.00557325,  0.01378215,  0.01220219,  0.01907699,  0.01854683,\n",
       "        0.01579614, -0.01397901, -0.01831173, -0.00071151, -0.00619968,\n",
       "        0.01578863,  0.01187715, -0.00309133,  0.00302193,  0.00358008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized_sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "cat_vector = model.wv['cat']\n",
    "\n",
    "cat_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1dea3b8b-f2d3-4080-89ae-798875f0c6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('notebook', 0.16563552618026733),\n",
       " ('on', 0.13940520584583282),\n",
       " ('the', 0.1267007440328598),\n",
       " ('ate', 0.08872982859611511),\n",
       " ('dog', 0.011071977205574512),\n",
       " ('my', -0.027841340750455856),\n",
       " ('sat', -0.03727477416396141),\n",
       " ('chased', -0.15515567362308502),\n",
       " ('mat', -0.2187293916940689)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_to_cat = model.wv.most_similar('cat')\n",
    "similar_to_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4607b6aa-a086-4da3-8b13-9cc07ba4fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 :  11314\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "print('총 샘플 수 : ',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9189a015-b21e-4066-8bd8-296bb4a1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace('[^a-zA-Z]', ' ') # 특수문자 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 길이가 3 이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower()) # 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "12722dd6-cff7-4daf-90cb-17080cce4613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cd1ed0e1-16dd-4269-a963-115cb7e4dd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace('', float('NaN'), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f6e7595a-10b4-4f54-aa39-d3fd2dd35458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 :  11004\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 : ', len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ced9486b-fc01-46bb-9278-fed285b19387",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2225baf4-df58-4c78-9b37-18be1a5ac67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 :  10961\n"
     ]
    }
   ],
   "source": [
    "# Indices of sentences with length 1 or less\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "\n",
    "# Remove those sentences from tokenized_doc\n",
    "tokenized_doc = [sentence for index, sentence in enumerate(tokenized_doc) if index not in drop_train]\n",
    "\n",
    "# Print the total number of remaining samples\n",
    "print('총 샘플 수 : ', len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2f466b9-7b5f-473c-8895-7a11a444fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[913, 19, 8, 9872, 1017, 39306, 2032, 2033, 497, 890, 63435, 375, 24007, 84, 63436, 3940, 25, 7921, 1526, 462, 8790, 999, 381, 3317, 847, 19, 63437, 455, 1998, 748, 15889, 64, 13041, 4231, 1640, 515, 288, 24008, 109, 2033, 28, 2816], [1226, 42, 8791, 6911, 4335, 1309, 8328, 1581, 2865, 39307, 28, 83, 3186, 259, 11172, 2865, 17773, 317, 39308, 4473, 3187, 781, 24009, 1662, 63438, 2385, 2458, 1403, 726, 163, 4033, 5710, 347, 20477, 24010, 106, 1999, 28, 573, 706, 816, 132, 1309, 1431, 2227, 39307, 882], [40, 3679, 533, 843, 471, 5345, 3852, 871, 199, 7520, 37, 9873, 63439, 731, 328, 5711, 902, 24011, 109, 4591, 3941, 120, 1192, 279, 20478, 883, 7521, 286, 5, 5923, 106, 1357, 3941, 63440, 39309, 139, 63441, 616, 12, 39310, 2180, 63442, 139, 7522, 20, 213, 39311, 913, 286, 24012, 286, 139, 4127, 303, 5712, 3941, 129, 2, 498, 29726, 162, 24013, 437, 2965, 305, 63443, 8, 60, 7523, 11173, 163, 90, 29727, 25, 210, 13, 13042, 305, 63444, 411, 102, 14287, 63445, 25, 63446, 8, 20479, 914, 3, 14288, 466, 412, 163, 370, 234, 1527, 20480, 851, 110, 4232, 1862, 33, 743, 69, 1137, 118, 1786, 39312, 3452, 914, 883, 112, 266, 328, 2082, 36, 787, 63447, 40, 63448, 29728, 91, 1192, 1128, 288, 8, 3853, 140, 156, 1641, 63449, 393, 15, 8329, 5924, 69, 63450, 13043, 63451, 15890, 1497, 1339]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
    "\n",
    "print(encoded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aba2bdea-77a0-4d7a-adfc-fc90c34688a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 개수 :  181839\n"
     ]
    }
   ],
   "source": [
    "print('단어 개수 : ', len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0bb852ad-4c7f-4d4d-af53-071427deed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=len(word2idx)+1, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fa649ba5-d762-48cf-9fa4-5060f6a0ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(front) (462), late (890)) -> 1\n",
      "(whatever) (515), embarassing.2nd.amendment (100673)) -> 0\n",
      "(60s/) (63435), day. (1017)) -> 1\n",
      "(know.) (847), addition, (1526)) -> 1\n",
      "(addition,) (1526), model (455)) -> 1\n"
     ]
    }
   ],
   "source": [
    "paris, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print('({:s}) ({:d}), {:s} ({:d})) -> {:d}'.format(\n",
    "        idx2word[paris[i][0]], paris[i][0],\n",
    "        idx2word[paris[i][1]], paris[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "48d00f74-17f3-41ed-86ac-1f808adc870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 :  10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 : ', len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2b58837e-da84-4bd8-b7fa-6973514822f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 1460\n"
     ]
    }
   ],
   "source": [
    "print(len(paris), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a2125d33-3923-48f8-9311-13d6bc4ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7f09f7a8-ed98-4442-864b-70b13f099cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 1, 100)               1818390   ['input_3[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 1, 100)               1818390   ['input_4[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                 (None, 1, 1)                 0         ['embedding_2[0][0]',         \n",
      "                                                                     'embedding_3[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1)                    0         ['dot_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1)                    0         ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36367800 (138.73 MB)\n",
      "Trainable params: 36367800 (138.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(len(word2idx)+1, dim)(w_inputs)\n",
    "\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding = Embedding(len(word2idx)+1, dim)(c_inputs)\n",
    "\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "490b3148-6bb6-47db-a77b-08f4f7318d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d0942409-3174-4da5-a01b-2e3a963ec63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 Loss :  6.931556463241577\n",
      "Epoch :  2 Loss :  6.931489825248718\n",
      "Epoch :  3 Loss :  6.93148010969162\n",
      "Epoch :  4 Loss :  6.931474685668945\n",
      "Epoch :  5 Loss :  6.931472718715668\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "\n",
    "    print('Epoch : ', epoch, 'Loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e279a88e-19b5-43cb-866f-19da9aeb019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('vectors.txt', 'w', encoding='utf-8')\n",
    "f.write('{} {}\\n'.format(len(word2idx), 100))\n",
    "vectors = model.get_weights()[0]\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a274f9b3-98c2-404b-bf75-4eb59a858cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stereogram', 0.48528584837913513),\n",
       " ('superpowers,', 0.44478344917297363),\n",
       " ('n_0!=za#;ss', 0.41870638728141785),\n",
       " ('sanctioning', 0.41490331292152405),\n",
       " (\"could've\", 0.40699246525764465),\n",
       " ('flowing', 0.3929716646671295),\n",
       " ('spart.par', 0.3913535475730896),\n",
       " ('disclaimed', 0.3847351372241974),\n",
       " ('pinched', 0.3830842971801758),\n",
       " ('\"effectively\".', 0.3813590705394745)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba1950-ad14-4a6b-b047-a512e54cb565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
